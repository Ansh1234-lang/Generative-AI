{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab22156d",
   "metadata": {},
   "source": [
    "#### Data Ingestion - Documentloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "650903f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x16e69640550>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## text loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecb8a252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document=loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f60ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c170fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading a pdf file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('attention.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a78e280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_path=(\"https://bollyflix.fish/sarzameen-2025-hindi-movie/\",),\n",
    "                        bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"main-container\", \"imdb_container\",\"post-single-content box mark-links entry-content\") \n",
    "                        ))\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://bollyflix.fish/sarzameen-2025-hindi-movie/'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\n🏠 HOME\\n🎬 MOVIES\\n\\nSOUTH HINDI DUBBED\\nADULT MOVIES\\nBOLLYWOOD\\nDUAL AUDIO\\nHINDI DUBBED\\nHOLLYWOOD\\nMULTI AUDIO\\nPUNJABI\\nBENGALI\\n\\n\\n🔗 GENRE\\n\\nACTION\\nADVENTURE\\nANIMATION\\nCOMEDY\\nCRIME\\nDRAMA\\nFANTASY\\nHISTORICAL\\nHORROR\\nMYSTERY\\nPOLITICAL\\nROMANCE\\nSCI-FI\\nTHRILLER\\n\\n\\n📅 YEAR\\n\\n2025\\n2024\\n2023\\n2022\\n2021\\n2016-2020\\n\\n2016\\n2017\\n2018\\n2019\\n2020\\n\\n\\n2011-2015\\n2006-2010\\n2001-2005\\n1880-2000\\n\\n\\n☠️ QUALITY\\n\\n480p MOVIES\\n720p MOVIES\\n1080p MOVIES\\n300MB MOVIES\\n500MB MOVIES\\n700MB MOVIES\\n900MB MOVIES\\n1GB MOVIES\\n\\n\\n📺 TV SHOWS\\n🌐 WEB SERIES\\n\\nNETFLIX\\nAMAZON PRIME VIDEO\\nKOREAN DRAMA\\nHINDI DUBBED SERIES\\nHOTSTAR\\nHULU\\nMX PLAYER ORIGINALS\\nVOOT SELECT\\nONGOING SERIES\\n\\n\\nANIME\\n \\n\\n\\n\\n\\n\\nHOME » MOVIES » BOLLYWOOD\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDownload Sarzameen (2025) Hindi Movie 480p | 720p | 1080p WEB-DL ESub \\n 6 days ago\\n\\n\\n\\n\\n✅Download Sarzameen (2025) Hindi Movie available in 1080p, 720p & 480p Qualities For Your Mobile/tablet/Computer. This movie is based on Drama, Thriller.\\nBollyFlix  Provide You With Super Quality Of Movies and WEB Series. We Provide Google Drive Direct Download Links For Fast And Secure Download. You Can Join us on Telegram For the Latest Updates.\\n\\n\\n\\n\\n?.?\\n\\n\\n\\n\\nSarzameen\\n | July 25, 2025 (India)\\n\\nSummary: An Army officer will stop at nothing to free Kashmir valley from terrorism, even if it means paying a terrible price.\\nCountries: IndiaLanguages: Hindi\\nSource: imdb.comDisclaimer: This plugin has been coded to automatically quote data from imdb.com. Not available for any other purpose. All showing data have a link to imdb.com. The user is responsible for any other use or change codes.\\n\\n\\nDownload Sarzameen (2025) Hindi Movie ~ BollyFlix\\nMovie Details :\\n\\nFull Name:\\xa0Sarzameen\\nLanguage: Hindi\\nReleased Year:\\xa02025\\nSize: 440MB | 730MB | 1.1GB | 2.3GB | 3.9GB\\nQuality:\\xa0480p | 720p | 1080p\\nSource: WEB-DL\\nGenres:\\xa0Drama, Thriller\\nCast:\\nFormat:\\xa0MKV\\nSubtitle: English\\n\\nStoryline:\\nAn Army officer will stop at nothing to free Kashmir valley from terrorism, even if it means paying a terrible price.\\nScreenshots: \\n\\n\\nSarzameen (2025) Hindi 480p [440MB] \\n🚀 Google Drive \\n 🔗 Download Links   \\nSarzameen (2025) Hindi 720p [1.1GB] \\n🚀 Google Drive \\n 🔗 Download Links   \\nSarzameen (2025) Hindi 1080p [3.9GB] \\n🚀 Google Drive \\n 🔗 Download Links   \\n.............................. \\n\\nSarzameen (2025) Hindi 720p (10bit) [730MB] \\n🚀 Google Drive \\n 🔗 Download Links   \\nSarzameen (2025) Hindi 1080p (10bit) [2.3GB] \\n🚀 Google Drive \\n 🔗 Download Links   \\n\\n\\n\\n\\n\\n\\n\\nWatch Trailer\\n\\n\\n\\n\\n\\n How To Download  Join on Telegram  \\n\\n\\n\\nRelated Posts \\n\\n \\n\\nDownload Lovelace (2013) Dual Audio [Hindi-English] Movie 480p | 720p | 1080p WEB-DL ESub\\n\\n March 8, 2025\\n\\n\\n\\n\\n\\n \\n\\nDownload Once Upon a Studio (2023) Dual Audio {Hindi-English} Movie 480p | 720p | 1080p WEB-DL ESub\\n\\n October 20, 2023\\n\\n\\n\\n\\n\\n \\n\\nDownload Practical Magic (1998) English Movie 480p | 720p BluRay ESub\\n\\n October 3, 2023\\n\\n\\n\\n\\n\\n \\n\\nDownload Them That Follow (2019) UNCUT Dual Audio {Hindi-English} Movie 480p | 720p | 1080p BluRay ESub\\n\\n May 28, 2024\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nAdd Comment Cancel Reply\\n\\n\\n Save my name, email, and website in this browser for the next time I comment.\\n \\n\\nΔ \\n\\n\\n\\n\\n\\nJoin On TelegramMOVIESBENGALIBHOJPURIBOLLYWOODCHINESEDUAL AUDIOFOREIGNGUJARATIHINDI DUBBEDHOLLYWOODJAPANESEKANNADAKOREANMALAYALAMMARATHIMULTI AUDIOOTHERSPUNJABISOUTH HINDI DUBBEDTAMILTELUGUTURKISHURDUWEB SERIESALT BALAJI EXCLUSIVEAmazon MiniTVAMAZON PRIME VIDEOApple TV+BBCCartoon SeriesDISNEY+ENGLISH SERIESEROS NOWFliz MoviesHBO MaxHINDI DUBBED SERIESHoichoiHOTSTARHULUJIOCINEMAKOREAN DRAMAMX PLAYER ORIGINALSNETFLIXONGOING SERIESPARAMOUNT+SONYLIVTHE CW NETWORKTVF PLAYULLU ORIGINALSViu OriginalsVOOT SELECTZEE5 EXCLUSIVEBollyFlix \\n\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs=ArxivLoader(query=\"1706.08386\",load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2017-06-23', 'Title': 'Flows, scaling, and the control of moment hierarchies for stochastic chemical reaction networks', 'Authors': 'Eric Smith, Supriya Krishnamurthy', 'Summary': 'Stochastic chemical reaction networks (CRNs) are complex systems which\\ncombine the features of concurrent transformation of multiple variables in each\\nelementary reaction event, and nonlinear relations between states and their\\nrates of change. Most general results concerning CRNs are limited to restricted\\ncases where a topological characteristic known as deficiency takes value 0 or\\n1. Here we derive equations of motion for fluctuation moments at all orders for\\nstochastic CRNs at general deficiency. We show, for the case of the mass-action\\nrate law, that the generator of the stochastic process acts on the hierarchy of\\nfactorial moments with a finite representation. Whereas simulation of\\nhigh-order moments for many-particle systems is costly, this representation\\nreduces solution of moment hierarchies to a complexity comparable to solving a\\nheat equation. At steady states, moment hierarchies for finite CRNs interpolate\\nbetween low-order and high-order scaling regimes, which may be approximated\\nseparately by distributions similar to those for deficiency-0 networks, and\\nconnected through matched asymptotic expansions. In CRNs with multiple stable\\nor metastable steady states, boundedness of high-order moments provides the\\nstarting condition for recursive solution downward to low-order moments,\\nreversing the order usually used to solve moment hierarchies. A basis for a\\nsubset of network flows defined by having the same mean-regressing property as\\nthe flows in deficiency-0 networks gives the leading contribution to low-order\\nmoments in CRNs at general deficiency, in a $1/n$-expansion in large particle\\nnumbers. Our results give a physical picture of the different informational\\nroles of mean-regressing and non-mean-regressing flows, and clarify the\\ndynamical meaning of deficiency not only for first-moment conditions but for\\nall orders in fluctuations.'}, page_content='Flows, scaling, and the control of moment hierarchies for stochastic chemical reaction\\nnetworks\\nEric Smith\\nEarth-Life Science Institute, Tokyo Institute of Technology,\\n2-12-1-IE-1 Ookayama, Meguro-ku, Tokyo 152-8550, Japan\\nDepartment of Biology, Georgia Institute of Technology, 310 Ferst Drive NW, Atlanta, GA 30332, USA\\nSanta Fe Institute, 1399 Hyde Park Road, Santa Fe, NM 87501, USA and\\nRonin Institute, 127 Haddon Place, Montclair, NJ 07043, USA\\nSupriya Krishnamurthy\\nDepartment of Physics, Stockholm University, SE- 106 91, Stockholm, Sweden\\n(Dated: October 1, 2018)\\nStochastic chemical reaction networks (CRNs) are complex systems which combine the features\\nof concurrent transformation of multiple variables in each elementary reaction event, and nonlinear\\nrelations between states and their rates of change. Most general results concerning CRNs are limited\\nto restricted cases where a topological characteristic known as deﬁciency takes value 0 or 1, implying\\nuniqueness and positivity of steady states and surprising, low-information forms for their associated\\nprobability distributions. Here we derive equations of motion for ﬂuctuation moments at all orders\\nfor stochastic CRNs at general deﬁciency.\\nWe show, for the standard base-case of proportional\\nsampling without replacement (which underlies the mass-action rate law), that the generator of the\\nstochastic process acts on the hierarchy of factorial moments with a ﬁnite representation. Whereas\\nsimulation of high-order moments for many-particle systems is costly, this representation reduces\\nsolution of moment hierarchies to a complexity comparable to solving a heat equation. At steady\\nstates, moment hierarchies for ﬁnite CRNs interpolate between low-order and high-order scaling\\nregimes, which may be approximated separately by distributions similar to those for deﬁciency-0\\nnetworks, and connected through matched asymptotic expansions. In CRNs with multiple stable\\nor metastable steady states, boundedness of high-order moments provides the starting condition for\\nrecursive solution downward to low-order moments, reversing the order usually used to solve moment\\nhierarchies.\\nA basis for a subset of network ﬂows deﬁned by having the same mean-regressing\\nproperty as the ﬂows in deﬁciency-0 networks gives the leading contribution to low-order moments\\nin CRNs at general deﬁciency, in a 1/n-expansion in large particle numbers. Our results give a\\nphysical picture of the diﬀerent informational roles of mean-regressing and non-mean-regressing\\nﬂows, and clarify the dynamical meaning of deﬁciency not only for ﬁrst-moment conditions but for\\nall orders in ﬂuctuations.\\nKeywords: Chemical Reaction Network, deficiency, stochastic processes, moment\\nhierarchies, Anderson-Craciun-Kurtz theorem\\nI.\\nINTRODUCTION\\nRandom walks of multiple independent particles on\\nordinary graphs are simple processes in several funda-\\nmental senses [1–3].\\nEach step involves a change of a\\nsingle degree of freedom, and for the base case of pro-\\nportional sampling without replacement, the rate law for\\nﬂuxes is linear in concentrations. Moreover, identiﬁca-\\ntion of topological properties of ordinary graphs which\\ndetermine characteristics of random-walk dynamics can\\ngenerally be carried out in polynomial time [1, 4].\\nJust the opposite is true of the stochastic processes\\nassociated with Chemical Reaction Networks (CRNs).\\nEach elementary reaction event (for a general network)\\ncan involve the concurrent conversion of multiple inputs\\ninto multiple outputs [5, 6], making the elementary net-\\nwork on which the reactions occur not an ordinary graph,\\nbut a directed multi-hypergraph [7, 8]. For the base case\\nof proportional sampling without replacement over reac-\\ntants, the rate law becomes nonlinear, leading generically\\nto possibilities for complex dynamics and multiple (stable\\nor metastable) steady states [9–11]. Moreover, identiﬁca-\\ntion of key topological properties such as shortest reac-\\ntion sequences connecting inputs to outputs or complete\\nsets of self-amplifying cycles, which aﬀect the character\\nof CRN dynamics, are known to be NP-hard problems\\nfor hypergraphs [12, 13].\\nAs a consequence, although stochastic processes with\\nthe essential features of CRNs are ubiquitous in biochem-\\nistry [14], systems biology [15], ecology [16], and epidemi-\\nology [17], and are thus of large practical and theoretical\\ninterest, few results exist for these systems [18–22] com-\\npared with the large literature that exists for random\\nwalks on ordinary graphs [1].\\nIn addition to systems\\nwhich clearly have CRN structure because the underlying\\nprocesses obey constraints of stoichiometry (the source of\\nconcurrency), the CRN framework is ﬂexible enough to\\nfurnish a representation for systems of broad interest to\\nnon-equilibrium thermodynamics such as the zero-range\\nprocess [23], where topological characteristics that are\\nknown to lead to simple steady states for CRNs can be\\nused to sieve for exactly solvable cases. Indeed, some of\\narXiv:1706.08386v1  [physics.chem-ph]  23 Jun 2017\\n2\\nthe key results that are known for stochastic CRNs [24]\\nwere motivated in part by earlier work of Kelly on a re-\\nlated class of queuing networks [25].\\nPartitioning oﬀthe simple sub-architecture in the core of\\ncomplex CRNs\\nA representation scheme for CRNs was made standard\\nby the work of Feinberg [9, 18], which separates the struc-\\nture of independent reaction events from the stoichiomet-\\nric relations that determine their action in the chemical\\nstate space. We will emphasize that, well beyond the use\\nmade by its original authors, this decomposition deﬁnes\\nthe fundamental partition in CRN architecture between a\\nsub-system with the same simplicity as the random walk\\non an ordinary graph, and the remainder of the CRN\\nconstraints responsible for concurrency, nonlinearity, and\\ntheir resulting complexity. Identifying the subsystem iso-\\nmorphic to a simple process is the key to decomposing\\nthe scaling behaviors in the moment hierarchies of CRNs,\\nand to distinguishing the roles of diﬀerent classes of ﬂows\\nin the dynamics and steady states.\\nThe emphasis in the work of Feinberg, and in closely-\\nrelated work by Horn and Jackson [26], was the exis-\\ntence and uniqueness of solutions to the mass-action rate\\nequations with strictly positive concentrations: a limited,\\ndeterministic, and static problem. We will show below\\nthat the Feinberg decomposition is even more useful in\\nthe analysis of the stochastic processes associated with\\nCRNs, where in addition to exact results in deterministic\\nlimits, it can serve as a foundation for systematic approx-\\nimation methods in the general case. A host of results\\nfollow, including novel representations of the generator\\nof the stochastic process acting on the moment hierar-\\nchy, duality relations of the kind explored in Stochastic\\nThermodynamics [27], and scaling relations that suggest\\nsolution methods using matched asymptotic expansions.\\nAn expanded role for deﬁciency\\nA new dimensional property termed deﬁciency was in-\\ntroduced by Feinberg [9], which was central to both the\\nresults on existence and uniqueness of steady states, and\\nto a limited but important corollary about the form of\\nsome of the distributions associated with such states. De-\\nﬁciency can be computed as a topological index from\\nthe graphical structure associated with a CRN [9–11],\\nbut its importance comes from its meaning as a count\\nof the dimensionality of chemical ﬂows that can pro-\\nceed in a steady state without being subject to mean-\\nregression due to changes in chemical concentrations.\\nWhen there are no such regression-free ﬂows – when de-\\nﬁciency equals zero – the network is guaranteed to have\\nunique strictly positive steady states at general parame-\\nters. A remarkable result due to Anderson, Craciun, and\\nKurtz (ACK) [24] is that under the same conditions, the\\nsteady-state distributions have a simple factorial form\\nunder proportional sampling rules, and a class of related\\nforms under more general rules (e. g., Michaelis kinetics),\\nas long as they sample chemical species independently of\\none another.\\nThe concept expressed by deﬁciency remains key to or-\\nganizing the stochastic processes for general CRNs, even\\nwhen their deﬁciency is nonzero, and the clues for why\\nthis should be so are already latent in the ACK theorem\\n[24]. For the simple case of proportional sampling with-\\nout replacement (which gives rise to mass-action kinetics,\\nand which we will assume in the remainder of the article),\\nthe ACK solutions are either products of Poisson distri-\\nbutions, or hypersurfaces within such product-Poissons\\nconstrained by particle conservation laws (with no loss\\nof generality for the claims below). In a Poisson distri-\\nbution, all higher moments are universal functions of the\\nmean value, so in a product-Poisson, the entire moment\\nhierarchy is controlled by the set of ﬁrst-moments. In a\\ndeﬁciency-zero network at steady state, the ACK theo-\\nrem eﬀectively states that ﬁrst-moment values carry all\\nthe “information” in the distribution. When deﬁciency\\nis nonzero, the mean-regressing ﬂows are no longer the\\nexclusive dynamical entities, but as we will show they\\nremain the dominant entities governing low-order mo-\\nments, in an asymptotic expansion where the small pa-\\nrameter is the order of any moment of the distribution\\nrelative to its mean particle number. The approach by\\nwhich we will construct this result also shows how to ex-\\ntract other scaling regimes associated with the remaining\\nﬂows in networks with non-zero deﬁciency, and the way\\nthese control complementary asymptotic expansions for\\nhigh-order moments relative to mean particle numbers.\\nCoupling CRN theory to Doi operator algebras for stochastic\\nprocesses\\nOur approach to the stochastic processes associated\\nwith CRNs grows out of a set of linear-algebra methods\\ndue to Masao Doi [28, 29], for the treatment of generating\\nfunctions for general discrete-state stochastic processes.\\nThe Doi operator algebra provides a starting point for\\nnumerous solution methods,1 but its simplifying eﬀect\\nis particularly elucidating for CRNs.\\nIn the classical\\nmass-action theorems of Feinberg, the rate equations for\\nCRNs take an awkward and not-very-perspicuous form,\\nin which stoichiometry is expressed asymmetrically in\\nnonlinear rate laws and concurrency constraints on in-\\nputs and outputs. In the Doi representation of the full\\nstochastic process, all formal asymmetry between inputs\\n1 One of the best-known of these is the coherent-state expan-\\nsion of generating functionals due to Luca Peliti [30, 31].\\nIt\\nis particularly useful for semiclassical approximations and other\\nstationary-point methods, which we mention but do not pursue\\nin depth here.\\n3\\nand outputs disappears. It can be seen that the asym-\\nmetry of the Feinberg problem reﬂects the way the par-\\nticular projection operator to the ﬁrst-moment equations\\nof motion interacts with the formally-symmetric genera-\\ntor of all ﬂuctuations. Within the framework of the Doi\\nalgebra, we formulate the projection operator that gives\\nthe equations of motion for arbitrary moments, and show\\nhow the Feinberg equations generalize to a new represen-\\ntation of the generator of the stochastic process acting on\\nthe moment hierarchy, which directly expresses the scal-\\ning inﬂuence of diﬀerent network ﬂows. The underlying\\nsymmetry of the Doi representation of the stochastic pro-\\ncess generator remains, and can serve as a point of depar-\\nture for the derivation of duality relations for stochastic\\nCRNs, which we mention here but develop in a separate\\npublication [32].\\nOrganization of the presentation\\nThe presentation is organized as follows:\\nIn Sec. II we introduce the general concepts and no-\\ntation associated ﬁrst with Chemical Reaction Networks,\\nthen with general discrete-state stochastic processes, and\\nﬁnally for the particular forms of stochastic processes as-\\nsociated with CRNs. The culmination of this section is\\nEq. (27), the Liouville-form expression for the genera-\\ntor of the stochastic process of a CRN motivated by the\\nFeinberg decomposition, which is the basis for all other\\nresults in the paper.\\nSec. III introduces the factorial moments which are the\\nnatural observables for CRNs with simple mass-action\\nrate laws, and shows that a dynamical equation for all\\norders of ﬂuctuations is closed and has a ﬁnite-order gen-\\nerator in this set of moments. The culmination of this\\nsection is the representation of the generator in Eq. (34).\\nSec. IV then shows how topological characteristics of\\nthe CRN are linked to dynamical properties of ﬂows, and\\nreviews the Feinberg deﬁciency-0 theorem and the asso-\\nciated Anderson-Craciun-Kurtz theorem. We introduce\\nwhat we term the stoichiometric decomposition of the\\nLiouville operator in Eq. (40), which separates two dy-\\nnamically diﬀerent classes of mean-regressing and non-\\nmean-regressing ﬂows.\\nSec. V uses the representation of the generator on the\\nlattice of factorial moments to show how diﬀerent com-\\nbinations of rate constants from a CRN govern scaling\\nproperties of moments in diﬀerent ranges of the moment\\norder. This section shows how matched asymptotic ex-\\npansions can be used to solve for steady-state moment\\nhierarchies recursively, and shows the (related) sense in\\nwhich the deﬁciency-0-like subset of ﬂows in the stoi-\\nchiometric decomposition dominate low-order moments.\\nSec. VI then provides a sequence of worked examples of\\nascending complexity to introduce each of the concepts\\nabove and show its eﬀects in a solution.\\nWe have chosen to develop all main results in their gen-\\neral forms in Sections II – V, in the interest of economy\\nand continuity of the argument, postponing examples to\\nSec. VI where they may be directly compared. The sim-\\nplest case and the starting point involves one species and\\ndeﬁciency zero (VI A), illustrating the way the ACK the-\\norem is recovered in the Doi algebra with a one-line proof.\\nWe then introduce nonzero deﬁciency while keeping the\\nsame mass-action equations of motion (VI B), to show\\nhow the subspace of ﬂows resembling a zero-deﬁciency\\nnetwork can be extracted (but also why, in the general\\ncase, this cannot be expressed as a zero-deﬁciency sub-\\nprocess of the full process), and how it controls the scal-\\ning of low-order moments. Next we hold deﬁciency ﬁxed\\nbut change the network topology to one in which auto-\\ncatalytic feedback produces multiple steady states in the\\nmass-action approximation (VI C). This case introduces\\nthe ﬁrst non-trivial role for the asymptotic expansion and\\nshows, counter-intuitively, how a criterion of bounded-\\nness for asymptotically high-order moments anchors the\\nrecursion downward to specify the low-order moments of\\nthe ergodic distribution over the two steady states. All\\neﬀects up to this point are illustrated with single-species\\nnetworks. At the end we introduce a two-species network\\nin which cross-catalysis replaces the single-species auto-\\ncatalysis (VI D), yielding an equivalent bistable classical\\nsystem if the species are not distinguished.\\nThis case\\ndemonstrates a non-trivial use of the stochastic process\\ngenerator acting on the moment hierarchy, and shows\\nhow the factorability of the ACK theorem for multi-\\nspecies distributions is lost at non-zero deﬁciency.\\nReaders who prefer to alternate general notation and\\ninstances are encouraged to browse the examples in\\nSec. VI in parallel with reading the formal development\\nin the earlier sections. A more direct track to the main\\nresults on recursions in the moment hierarchy is also pro-\\nvided in [33].\\nII.\\nCHEMICAL REACTION NETWORKS AND\\nTHEIR ASSOCIATED STOCHASTIC\\nPROCESSES\\nThe next three sub-sections review the standard con-\\ncepts for CRNs and two aspects of stochastic-process al-\\ngebras – representations of the generator and then the\\nDoi operator formalism – and introduce the notation in\\nwhich we will represent them in this paper. In Sec. II D\\nthese are then brought together to obtain the Liouville-\\noperator representation of the generator for a stochastic\\nprocess CRN that will be the basis for all further con-\\nstructions.\\nA.\\nThe elements of a CRN\\nOur decomposition of CRNs follows that of Fein-\\nberg [9], but we use a more complete graphic represen-\\ntation, illustrated for our simplest example process in\\nFig. 1. We will present CRNs using both this graphic\\n4\\nα\\nβ\\nFIG. 1:\\nGraphical representation for a minimal CRN model\\nand the ﬁrst example in the sequence that will be developed in\\nSec. VI. One chemical species, two complexes, a single linkage\\nclass, and deﬁciency zero.\\nThese terms are deﬁned in the\\nremainder of the section.\\nform, and a corresponding reaction-scheme form such as\\nA\\nα−⇀\\n↽−\\nβ 2A.\\n(1)\\nThe following list explains the fundamental division of\\nFeinberg, Horn and Jackson that separates the so-called\\ncomplex network 2 from the stoichiometric relations that\\ninterpret its action in the state space of chemical species:\\nThe chemical species: In examples we will denote ex-\\nplicit species names with single capital Roman letters\\nsuch as A. Where a set of species is indicated, we index\\nthem with subscripts p ∈1, . . . , P, the number of dis-\\ntinct chemical species. In network diagrams a species is\\ndenoted with a ﬁlled dot:\\nComplex: a multiset of species, which is the input or\\noutput of a reaction. It may be written as a sum rather\\nthan a set. Examples might be A, 2A (or equivalently\\nA + A). In network diagrams, a complex is denoted by\\nan open circle with one or more (labeled) dashed line-\\nstubs indicating the reaction(s) in which it participates\\nand (labeled) solid line-stubs indicating the participating\\nspecies:\\n(In one-species or one-reaction networks,\\nor after the network diagram has been assembled, we\\nmay suppress the labels to improve readability.) In set\\nnotation, we index complexes with subscripts i or j.\\n(Directed) reaction: an ordered pair of complexes with\\nan associated rate constant. For complexes indexed i and\\nj, respectively, the reaction from i to j would correspond\\nto the ordered pair (i, j), and the associated rate constant\\nis denoted kji. (For simple models, rate constants may\\nbe given simplifying labels such as α.)\\nAn expression with one or more reaction is known as\\na reaction scheme, such as\\nA\\nα⇀2A.\\n(2)\\nIn network diagrams, a reaction is represented with a\\ndashed arrow between the input and output complexes\\n(optionally labeled with the rate constant):\\nα\\n2 Here the stress is on the ﬁrst syllable – \\'com,plex network – to\\nbe distinguished from references to systems that are com\\'plex.\\nChemical Reaction Network (CRN): a collection of\\nreactions.\\nBy default we adopt the ﬁnest-grained de-\\nscription in which all reactions are unidirectional; bi-\\ndirectional reactions are indicated with pairs of directed\\narrows. In reaction schemata we may also condense no-\\ntation for bidirectional reactions, as above in Scheme 1.\\nGraphically a CRN is a well-formed doubly-bipartite\\nnetwork (two kinds of nodes and two kinds of links), in\\nwhich all reactions terminate in two complexes, and all\\nline-stubs from a complex are ﬁlled by the appropriate\\nreactions or links to chemical species3, as in Fig. 1.\\nAdjacency/rate matrix: In Feinberg’s representation\\nof CRNs [9, 18], only the complexes and the reactions are\\ndenoted explicitly, and they form an ordinary, directed\\ngraph. In a stochastic formulation, reactions can occur as\\nindependent events on the links, analogous to the steps in\\na simple random walk. Complexes are treated as if they\\nhave activities, and the set of rate constants map these\\nactivities to reaction rates. In this way, both stoichiomet-\\nric constraints and the determination of activities of com-\\nplexes are cordoned oﬀas separate information from the\\nrate- and connectivity-structure of the complex-network.\\nThe latter is given by an ordinary adjacency/rate ma-\\ntrix, identical in form to the graph Laplacian for a simple\\nrandom walk. Following the notation made standard by\\nFeinberg, we denote this matrix by A.\\nArranging complexes in a column vector indexed by\\ni, let wi be the indicator function that is nonzero on\\ncomplex i only (so the jth component (wi)j ≡δij), and\\nwT\\ni\\nits transpose which can act as a projection opera-\\ntor. Then the adjacency/rate matrix can be written as\\na sum of dyadics, arranged in a variety of ways. Two of\\nthese we call the reaction representation and the complex\\nrepresentation, written\\nA =\\nX\\n(i,j)\\n(wj −wi) kjiwT\\ni\\nreaction rep.\\n=\\nX\\ni\\nwi\\nX\\nj\\n\\x00kjiwT\\nj −kijwT\\ni\\n\\x01\\ncomplex rep.\\n(3)\\nIn expressions such as the canonical mass-action rate law\\n– which will appear as Eq. (41) below, after its context\\nhas been properly introduced – the row vectors wT select\\nthe activities determining reaction rates, and the column\\nvectors w identify the net ﬂux into and out of complexes.\\nThe reaction representation is often more intuitive for\\nconstructing generators of the stochastic process, while\\nthe complex representation, in accumulating all ﬂows into\\n3 Strictly speaking, a CRN corresponds to a directed multi-\\nhypergraph, in which the reactions correspond to directed hyper-\\nedges, and the input and output complexes are their vertex sets.\\nThe graphic depiction used here and elsewhere is a bipartite rep-\\nresentation of the underlying hypergraph.\\nThe computational\\ncomplexity of numerous search and optimization problems on\\nhypergraphs, which are simple on ordinary graphs, is one conse-\\nquence of the concurrency of inputs and outputs on a hyperedge.\\n5\\nor out of a complex, more directly reﬂects the cause of\\nmean-regression that underlies the concept of deﬁciency.\\nStoichiometric matrix: In order to connect complexes\\nto species, treat n ≡[np] as a column vector, and intro-\\nduce a matrix Y with rows yp ≡\\n\\x02\\nyi\\np\\n\\x03T . yi\\np are the stoi-\\nchiometric coeﬃcients indicating the number of instances\\nof species p in complex i. In graphs such as Fig. 1, this\\nis the number of solid lines from the species node p to\\nthe complex node i. The product Y A connects ﬂuxes at\\ncomplexes (the row index on A) to ﬂuxes at species (the\\nrow index on Y A).\\nInterpretation of complex activities: Complexes are\\nnot the same as chemical species, and their (virtual) ac-\\ntivities in the Feinberg complex-graph must be ﬁxed in\\nterms of the (actual) activities of the species. We will call\\nany such dependence an interpretation of the complexes.4\\nTo give an interpretation of complex activities in terms\\nof species activities that is convenient to use with the ad-\\njacency/rate matrix, let ΨY ≡\\n\\x02\\nΨi\\nY\\n\\x03\\nbe a column vector\\nwith components given by\\nΨi\\nY (n) ≡\\nY\\np\\nnp!\\n\\x00np −yip\\n\\x01\\n!\\n(4)\\nΨY deﬁnes the activity products corresponding to pro-\\nportional sampling without replacement on the discrete\\nindices np. We will return below to the relation between\\ndiscrete sampling and mass-action rate laws, after we\\nhave introduced notation and concepts for the stochastic\\nprocess that governs mass action and all higher moments.\\nB.\\nRepresentations of a stochastic process\\nA certain standard machinery underlies all discrete-\\nstate stochastic processes, including those associated\\nwith CRNs. Before deriving the particular forms for the\\nnetwork decomposition of Sec. II A, we introduce general\\nnotation and constructions in this section.\\nThe point we wish to emphasize is that, while the\\nstochastic process for a CRN has a uniquely-deﬁned gen-\\nerator, that generator may have many representations,\\ndepending on whether we solve the stochastic process for\\nits probability density, or the moment-generating func-\\ntional of that density, or the hierarchy of moments eval-\\nuated directly.\\nWhen the activities of complexes are deﬁned by pro-\\nportional sampling without replacement as in Eq. (4) (the\\nsimplest case, corresponding to ideal gases or solutions),\\nthe moments that appear naturally in all rate equations\\n4 It is an interesting question, which we leave for other work,\\nwhether the random walk on the complex network can be repre-\\nsented formally in terms of a linear algebra of pseudo-particles,\\nwhich are formal proxies for the products of operators represent-\\ning real particles of the chemical species.\\nare what we term factorial moments, and it is for these\\nthat the equations of motion take the most compact form.\\nThe essential components in the stochastic-process de-\\nscription are then the following:\\nProbability density function and transfer matrix:\\nStates of the CRN are indexed by values of the vector\\nn, and reactions are treated as instantaneous changes of\\nstate.5\\nOur starting point in describing the stochastic\\nprocess is a probability density function ρn indexed on\\nthe values of n.\\nThe density ρ evolves on a time coordinate τ under a\\nmaster equation\\n∂ρ\\n∂τ = Tρ\\nshorthand for\\n∂ρn\\n∂τ =\\nX\\nn′\\nTnn′ρn′.\\n(5)\\nThe matrix T ≡[Tnn′] is called the transfer matrix, and\\nis one representation of the generator of the stochastic\\nprocess.\\nMoment-generating function and Liouville oper-\\nator: The moment-generating function is formed from\\nρn with the introduction of a vector z ≡[zp] of complex\\ncoeﬃcients, as the Laplace transform\\nφ(z) ≡\\nX\\nn\\n Y\\np\\nznp\\np\\n!\\nρn.\\n(6)\\nThe generating function evolves under a Liouville equa-\\ntion of the form\\n∂φ\\n∂τ = −Lφ\\nshorthand for\\n∂\\n∂τ φ(z) = −L\\n\\x12\\nz, ∂\\n∂z\\n\\x13\\nφ(z) .\\n(7)\\nL is called the Liouville operator. Its form is deﬁned from\\nthe transfer matrix in Eq. (5), and it provides an alter-\\nnative representation of the generator of the stochastic\\nprocess, acting on Laplace transforms.\\nExpectations\\nand\\nmoments\\nand\\ntheir\\ntime-\\ndependence: The expectation of an arbitrary function\\nO(n) (for “Observable”) of the components of n, in the\\nbackground ρ, is denoted\\n⟨O(n)⟩≡\\nX\\nn\\nO(n) ρn\\n(8)\\nSince ρ may be a continuous-valued quantity whereas n is\\ndiscrete, we introduce a particular short-hand with math-\\nItalic font for the ﬁrst moment\\nn ≡⟨n⟩=\\nX\\nn\\nnρn,\\n(9)\\n5 This level of coarse-graining in the description of reaction events\\nis the standard assumption also in Stochastic Thermodynam-\\nics [27].\\n6\\nwhich may vary continuously if ρ does. The mass-action\\nrate equations are expressed entirely in terms of n. De-\\npending on the form of the underlying distribution ρ and\\nwhether higher-order correlations can be expressed as\\nfunctions of n, the mass-action equations may be exact\\nor they may involve a (generally-unregulated) approxi-\\nmation known as the mean-ﬁeld approximation.\\nn may also be obtained from the generating function\\nas\\nn = ∂\\n∂z log φ(z)\\n\\x0c\\x0c\\x0c\\x0c\\nz≡1\\n,\\n(10)\\nand expectations of more complex observables can be\\nbuilt up by acting on φ appropriately with higher-order\\nderivatives in z.\\nThe time-dependence of n, which is the object of clas-\\nsical ﬁrst-moment equations or chemical rate equations,\\ncan be obtained by acting on either ρ or φ with its cor-\\nresponding generator, by Equations (5) or (7).\\nFactorial moments for CRNs: In many applications,\\nthe natural moments for which to study dynamics are\\neither ordinary powers nk, obtained by acting on φ with\\nhigher derivatives in z, or cumulants [34], obtained by\\nacting on log φ with higher derivatives in z. For CRNs\\nwith reaction rate laws corresponding to proportional\\nsampling without replacement, however, the simplest dy-\\nnamical relations are obtained for the factorial moments,\\nfor which we therefore introduce a speciﬁc notation. For\\na single component np and power kp,\\nn\\nkp\\np ≡\\nnp!\\n(np −kp)!\\n; kp ≤np\\n≡0\\n; kp > np.\\n(11)\\n(We adopt the convention in the second line of Eq. (11)\\nfor kp > np because it allows a simpliﬁcation in sum nota-\\ntions below.) We will use the form of Eq. (11) as a general\\nformula for truncated factorials, allowing other integer-\\nvalued arguments such as stoichiometric coeﬃcients to\\ntake the place of np as the argument.\\nMoment hierarchy and a new representation for\\nthe generator: For a vector k ≡[kp] of powers, we\\nintroduce the factorial moment hierarchy indexed by k,\\nas the expectation\\nΦk ≡\\n*Y\\np\\nn\\nkp\\np\\n+\\n.\\n(12)\\nWe show in Sec. III that Φ evolves in time as\\n∂Φ\\n∂τ = ΛΦ\\nshorthand for\\n∂Φk\\n∂τ\\n=\\nX\\nk′\\nΛkk′Φk′.\\n(13)\\nThe matrix Λ ≡[Λkk′] has ﬁnitely many nonzero en-\\ntries determined by the stoichiometric coeﬃcients, and\\nits form may be derived from the Liouville operator L.\\nΛ provides yet a third representation of the generator of\\nthe stochastic process, which is particularly well-suited\\nto the study of CRNs because it exposes diﬀerent scaling\\nregimes controlled by combinations of the rate param-\\neters corresponding to diﬀerent ﬂows. We develop the\\nimplications of scaling in Sec. V.\\nC.\\nThe Doi operator algebra for a discrete-state\\nstochastic process\\nFor most of its purposes as a generating function, it\\nis not necessary that φ(z) be an analytic function of a\\nvector z of complex-valued arguments. Often only the\\nformal power series in z, and its algebra with the deriva-\\ntive ∂/∂z, is required.\\nThe abstraction of the linear algebra of generating\\nfunctions in terms of formal raising and lowering oper-\\nators follows a procedure due to Masao Doi [28, 29]. We\\nhave elaborated the details of the mapping and its inter-\\npretation extensively elsewhere [35–37], and here we only\\nsummarize the notation, which by now is standard.6\\nThe Doi algebra denotes the argument-variables and\\ntheir derivatives as abstract raising and lowering opera-\\ntors,\\nzp →a†\\np\\n∂\\n∂zp\\n→ap,\\n(14)\\nbecause partial diﬀerentiation then imposes on these op-\\nerators the conventional commutation algebra\\n\\x02\\nap, a†\\nq\\n\\x03\\n= δpq,\\n(15)\\nwhere δpq is the Kronecker δ.\\nGenerating functions, which are polynomials multiply-\\ning the number 1, are written as the action of the raising\\noperators on a formal right-hand null state, while the\\nprojection operator that takes the trace of a generating\\nfunction with an integral is written as a left-hand null\\nstate:\\n1 →|0)\\nZ\\ndPz δP (z) →(0|\\n(16)\\nwhere δP (z) is the Dirac δ in P dimensions, and the inner\\nproduct of the null states is normalized: (0 | 0) = 1.\\nThe basis for generating functions is the set of number\\nstates which are elementary monomials. For any vector\\nn,\\nP\\nY\\np=1\\nznp\\np × 1 →\\nP\\nY\\np=1\\na†\\np\\nnp |0) ≡|n) ;\\n(17)\\n6 Indeed,\\nmost treatments open directly with the Doi alge-\\nbra [38, 39].\\nWe have used the two-step introduction by way\\nof conventional analytic generating functions because it clariﬁes\\nthe meaning of some terms in the Doi algebra that can be obscure\\nwhen presented without introduction.\\n7\\nnumber states are eigenstates of the set of number oper-\\nators a†\\npap:\\na†\\npap |n) = np |n) .\\n(18)\\nIn particular for use with CRNs, we note the role of low-\\nering operators in extracting the truncated factorials of\\nnumber arguments: for any non-negative integer k,\\nak\\np |n) = nk\\np |n −kp)\\na†\\np\\nkak\\np |n) = nk\\np |n) .\\n(19)\\nwhere |n −kp) is the number state with k subtracted\\nfrom np and all nq for q ̸= p unchanged.\\nWith these steps the generating function becomes a\\nvector in a linear space:\\nφ(z) =\\nX\\nn\\nP\\nY\\np=1\\nznp\\np ρn →\\nX\\nn\\nρn |n) ≡|φ) .\\n(20)\\nAll number states are normalized with respect to the\\nGlauber inner product, deﬁned by\\n(0| e\\nP\\np ap |n) = 1,\\n∀n,\\n(21)\\nand the Glauber inner product with a generating function\\nis simply the trace of the underlying probability density:\\n(0| e\\nP\\np ap |φ) =\\nX\\nn\\nρn = 1.\\n(22)\\nThe above conventions deﬁne the standard representa-\\ntion in which we will work with generating functions in\\nthe remainder of this article.\\nD.\\nThe stochastic process for a CRN\\nWe now apply the above operator formalism to the\\nparticular forms of transfer matrices and generating func-\\ntions produced by CRNs, and show how the reaction and\\ncomplex representations decompose the resulting repre-\\nsentations of the generators.\\nCorresponding to the column vector ΨY\\nof activi-\\nties from Eq. (4), we introduce a second column vector\\nψY ≡\\n\\x02\\nψi\\nY\\n\\x03\\nwhich takes as its argument the Doi lowering\\noperators ap that extract the truncated factorials of np\\naccording to Eq. (19). Making use of the notation (11) for\\nthese truncated factorials, we may write the coeﬃcients\\nin these two vectors as\\nΨi\\nY (n) ≡\\nY\\np\\nn\\nyi\\np\\np\\nψi\\nY (a) ≡\\nY\\np\\na\\nyi\\np\\np\\n(23)\\nThere is a corresponding row vector for the adjoints:\\nψ†\\nY ≡\\nh\\nψ†i\\nY\\niT\\nis a row vector of components deﬁned on\\nthe complex indices i, where the notation means\\nψ†i\\nY\\n\\x00a†\\x01\\n≡\\nY\\np\\na†yi\\np\\np\\n(24)\\n(As we did with the notation (11) for factorial moments,\\nwe take the deﬁnitions (23,24) for ψ and ψ† as general\\nforms, in which other arguments besides a and a†, such as\\nthe ﬁrst-moment value n can appear. This is convenient\\nwhen expressing the approximation made in the mass-\\naction rate law, and exhibiting its relation to the exact\\nequation of motion as an operator expression.)\\nWe now use the above notations to group the sample\\nnumbers and index shifts that describe the concurrent\\nconversion of reactants into products in the elementary\\nreaction events of a CRN. We begin with the transfer\\nmatrix, and then show the simpliﬁcations aﬀorded by\\nworking with the Liouville operator.\\nFor proportional sampling without replacement, the\\nnumber dependence of the probability for the reaction\\n(i, j) from state n is simply given by the component\\nΨi\\nY (n) from Eq. (23).\\nThe way the master equation\\nacts on indices is slightly more complicated: For the de-\\nlivery of probability into state n, the master equation\\nmust sample both ρ and Ψi\\nY at a value shifted from n by\\nthe stoichiometric coeﬃcients that are consumed at com-\\nplex i minus those that are produced at complex j. The\\nsimplest way to express such shifts is to let the vector\\nof shift operators e∂/∂n (with the exponential evaluated\\ncomponent-wise on n) serve as an argument to ψi\\nY , thus:\\nψi\\nY\\n\\x10\\ne∂/∂n\\x11\\n≡\\nY\\np\\neyi\\np∂/∂np = eyiT ∂/∂n;\\n(25)\\nwith this convention for both the rate constant and the\\nshift operator, the matrix T from Eq. (5) can be written\\nT = ψ†\\nY\\n\\x10\\ne−∂/∂n\\x11\\nA\\nh\\nψY\\n\\x10\\ne∂/∂n\\x11\\n· ΨY (n)\\ni\\n=\\nX\\n(i,j)\\nh\\nψj\\nY\\n\\x10\\ne−∂/∂n\\x11\\n−ψi\\nY\\n\\x10\\ne−∂/∂n\\x11i\\nkjiψi\\nY\\n\\x10\\ne∂/∂n\\x11\\nΨi\\nY (n) .\\n(26)\\nHere the dot-product (·) between ψY\\n\\x00e∂/∂n\\x01\\nand ΨY (n)\\nin the ﬁrst line indicates that these two vectors are to be\\nmultiplied component-wise with respect to the complex\\nindex i, so that their product is then extracted by the\\nindicator functions wT\\ni\\nin A from Eq. (3).\\nThe cross-\\nterm in ψj\\nY and ψi\\nY Ψi\\nY , made explicit in the second line\\nof Eq. (26), performs the required index shift on n in\\nboth ρ and ΨY to account for the particles lost from the\\nsystem’s state through complex i and those gained by the\\nsystem’s state through complex j. The other cross-term,\\nwith ψi\\nY\\n\\x00e−∂/∂n\\x01\\n, simply cancels the shift operators in\\nψi\\nY\\n\\x00e∂/∂n\\x01\\nand represents the loss of probability from\\nstate n with rate Ψi\\nY (n).\\n8\\nWorking with the Liouville operator from Eq. (7) is\\nmuch more straightforward, because the lowering oper-\\nators ap both extract sample numbers and shift indices\\naccording to Eq. (19), so these do not need to be sepa-\\nrately tracked as they are in the transfer matrix. Using\\nthe exact deﬁnitions (23,24) with the arguments a and\\na† implicit, L takes the form\\n−L = ψ†\\nY AψY\\n(27)\\nEq. (27) is one of the central equations of this\\npaper, and underlies many of the simpliﬁcations\\nwe present here.\\nIn this expression, all the formal\\nasymmetry of the standard ﬁrst-moment rate equations\\nfrom the CRN literature has disappeared, and particle\\nconsumption and creation are now treated symmetrically.\\nThis is the ﬁrst of many simpliﬁcations gained by working\\nwith the Laplace transform and the Doi operator algebra.\\nIII.\\nTHE DYNAMICS OF MOMENT\\nHIERARCHIES\\nFrom the foregoing constructions we can directly com-\\npute the equations of motion for arbitrary moments of\\nthe density ρn. These equations are ﬁnitely generated if\\nwe work in a basis of factorial moments, the demonstra-\\ntion of which is the main result of this section.\\nA.\\nDynamics of factorial moments for a single\\nspecies\\nFor a non-negative integer k, the operator that extracts\\nthe truncated factorial nk\\np from number-states, which are\\nthe basis for a general state vector |φ), is ak\\np. Therefore\\nthe expectation of nk\\np in the state |φ) is given by\\n\\nnk\\np\\n\\x0b\\n= (0| e\\nP\\nq aqak\\np |φ) ,\\n(28)\\nand from Eq. (7) and the form (27) for L, its time-\\ndependence is given by\\n∂\\n∂τ\\n\\nnk\\np\\n\\x0b\\n= (0| e\\nP\\nq aqak\\np (−L) |φ)\\n= (0| e\\nP\\nq aqak\\npψ†\\nY\\n\\x00a†\\x01\\nAψY (a) |φ) .\\n(29)\\nIn order to obtain a recursion relation for the time de-\\npendence of\\nD\\nnk\\np\\nE\\nin terms of the values of other factorial\\nmoments, we must commute the product of lowering op-\\nerators ak\\np through all powers of raising operators, which\\nare gathered in the coeﬃcients of the row-vector ψ†\\nY . The\\nresult of the commutation is a ﬁnite series with descend-\\ning powers of ap and a†\\np. For positive integers k and y,\\nthe evaluation of operator products of powers of raising\\nand lowering operators is given by7\\nak\\npa†y\\np =\\nmin(k,y)\\nX\\nj=0\\nk!y!\\nj! (k −j)! (y −j)! a†\\np\\ny−jak−j\\np\\n=\\nk\\nX\\nj=0\\n\\x12\\nk\\nj\\n\\x13\\nyj a†\\np\\ny−jak−j\\np\\n.\\n(30)\\nThe ﬁrst line emphasizes the symmetric roles of k and\\ny in the combinatorial coeﬃcient. In the second line we\\nhave used the deﬁnition (11) applied to yj (rather than\\nnj) to simplify the index of summation in the case that\\nk > y.\\nNow we may expand the evaluation appearing in\\nEq. (29), using the sum (30)\\n(0| e\\nP\\nq aqak\\npψ†i\\nY\\n\\x00a†\\x01\\n= (0|\\nk\\nX\\nj=0\\n\\x12\\nk\\nj\\n\\x13 \\x00yi\\np\\n\\x01j e\\nP\\nq aqak−j\\np\\n=\\nk\\nX\\nj=0\\n\\x12\\nk\\nj\\n\\x13 \\x00yi\\np\\n\\x01j (0| e\\nP\\nq aqak−j\\np\\n,\\n(31)\\nas an operator identity acting on general states. Here\\nwe have used the property of the Doi operator algebra\\nthat commutation through the exponential e\\nP\\nq aq shifts\\nall a†\\np →\\n\\x00a†\\np + 1\\n\\x01\\n, after which all factors of a†\\np annihi-\\nlate the right ground state (0|. Thus we have eliminated\\nall factors of a†, along the way extracting from ψ†\\nY the\\nfactorial moments\\n\\x00yi\\np\\n\\x01j of the stoichiometric coeﬃcients.\\nThe powers of the lowering operator ak−j\\np\\nin Eq. (31)\\nare the same form as terms already in ψY , so we can\\nabsorb them into ψY by shifting the stoichiometric coef-\\nﬁcients in row p, which we denote as\\n(0| e\\nP\\nq aqak−j\\np\\nψY (a) |φ) = (0| e\\nP\\nq aqψY +(k−j)p(a) |φ)\\n=\\nD\\nΨY +(k−j)p(n)\\nE\\n(32)\\nY + (k −j)p is the matrix in which the i\\np component is\\nyi\\np + k −j, ∀i, and yi\\nq is unchanged for q ̸= p.\\nFrom these evaluations we can re-express Eq. (29) as\\n∂\\n∂τ\\n\\nnk\\np\\n\\x0b\\n=\\nk\\nX\\nj=0\\n\\x12\\nk\\nj\\n\\x13\\nY\\nj\\np A\\nD\\nΨY +(k−j)p(n)\\nE\\n=\\nk\\nX\\nj=1\\n\\x12\\nk\\nj\\n\\x13\\nY\\nj\\np A\\nD\\nΨY +(k−j)p(n)\\nE\\n≡\\nk\\nX\\nj=1\\n\\x12\\nk\\nj\\n\\x13\\nY\\nj\\np e(k−j) ∂/∂YpA ⟨ΨY (n)⟩\\n(33)\\n7 The proof is by induction. If k ≥y, start with an elementary\\nevaluation of ak\\npa†p and then induct on y. If k ≤y, start with\\nan elementary evaluation of apa†y\\np and induct on k.\\n9\\nHere Y\\nj\\np is a row vector in which the ith component is the\\nfactorial moment\\n\\x00yi\\np\\n\\x01j. Note that Y 0\\np is the row vector\\nof 1s, Y 1\\np = Yp, the pth row of Y , etc. The ﬁrst line of\\nEq. (33) contains the full sum over j from Eq. (31), and\\nthe second line uses the fact that 1T A ≡0 to eliminate\\nthe j = 0 term. In the third line, we have expressed the\\nshift of coeﬃcients in the pth row of Y again using an\\nexponential shift operator denoted e∂/∂Yp, which acts on\\nall components yi\\np. This will be a convenient notation for\\nworking with moments involving multiple species.\\nB.\\nThe generator of the stochastic process acting\\non the moment hierarchy\\nFor multiple species, we generalize the notation to an\\ninteger-valued vector of powers k ≡[kp], and arrange\\nsummation indices similarly in vectors j ≡[jp]. Recalling\\nthe deﬁnition (12) of the factorial moment hierarchy Φ,\\nwe write its time derivative as\\n∂\\n∂τ Φk ≡∂\\n∂τ\\n*Y\\np\\nn\\nkp\\np\\n+\\n= ˙Y\\np\\n\\uf8ee\\n\\uf8f0\\nkp\\nX\\njp=0\\n\\x12\\nkp\\njp\\n\\x13\\nY\\njp\\np e(k−j)p ∂/∂Yp\\n\\uf8f9\\n\\uf8fbA ⟨ΨY (n)⟩\\nexpands to =\\nk1\\nX\\nj1=0\\n\\x12\\nk1\\nj1\\n\\x13\\n. . .\\nkP\\nX\\njP =0\\n\\x12\\nkP\\njP\\n\\x13 \"\\n˙Y\\np\\nY\\njp\\np\\n#\\nA\\n\\nΨY +(k−j)(n)\\n\\x0b\\n≡\\nX\\nk′\\nΛkk′Φk′.\\n(34)\\nThe notation ˙Q\\np denotes a product over species p within\\neach index i of the row vectors Y\\njp\\np . (Note that in the\\nsums over jp, we must now retain the jp = 0 entries,\\nbecause even if one index Y\\njp\\np\\n= [1]T , there may be others\\nin the sum where jp′ ̸= 0, and the product\\n\\x10 ˙Q\\npY\\njp\\np\\n\\x11\\nA\\nis only assured to vanish when all jp = 0.) Now the shift\\noperator e(k−j)p ∂/∂Yp in the ﬁrst line of Eq. (34) oﬀsets\\nall coeﬃcients yi\\np in the row p by (k −j)p, so we write\\nthe matrix Y in the second line with its rows shifted\\nuniformly by the entries in the column vector (k −j).\\nThe third line of Eq. (34) gives the deﬁnition of the\\nmatrix Λ introduced in Eq. (13). Each entry in the ma-\\ntrix ΨY +(k−j)(n) is itself a truncated factorial, so the\\nexpression is closed on Φ. Although the orders kp may\\nbe arbitrarily large, the matrix Λ has only ﬁnitely-many\\nnonzero entries, limited by the largest values of jp for\\nwhich the rows Y\\njp\\np\\nhave non-vanishing entries (recalling\\nthe deﬁnition (11) for truncated factorials).\\nEq. (34) is the main result with which we will\\nwork in this paper.\\nIt contains the rate equations\\nfor the species numbers n studied by Feinberg, Horn and\\nJackson, and extends these to give a compact representa-\\ntion for the dynamics of all higher-order moments as well.\\nIn Sec. VI we illustrate graphic methods for representing\\nΛ.\\nIV.\\nNETWORK TOPOLOGY, DEFICIENCY,\\nAND THE CLASSIFICATION AND ROLE OF\\nDIFFERENT NETWORK FLOWS\\nThe results up to this point are true for a general\\nCRN. Thus they say nothing directly about the topo-\\nlogical properties that may aﬀord simpliﬁcations such as\\nuniqueness and positivity of steady states or factorability\\nof distributions. In this section we shift to a consideration\\nof topology and its implications, including the concept of\\ndeﬁciency and the Feinberg [18] and Anderson-Craciun-\\nKurtz (ACK) theorems [24]\\nA.\\nDeﬁciency, and a basis to decompose network\\nﬂows\\nThe connectivity of the adjacency matrix A, together\\nwith the stoichiometric matrix Y , determines the lin-\\near subspace of n-values that can be accessed through\\nany ﬂow on the network, called the stoichiometric sub-\\nspace. The number and character of steady states de-\\npends on whether the n-dependence of the mass-action\\nrate equations within this subspace admits a Lyapunov\\nfunction [11, 26]. That, in turn, depends on whether all\\nﬂows that transport net matter into or out of any com-\\nplex must also transport some net matter into or out of\\nsome chemical species, thus increasing its chemical poten-\\ntial in a direction that opposes the ﬂow. If so, then all\\nthe ﬂows are mean-regressing and strictly positive steady\\nstates are unique. If not, there are net ﬂuxes at the com-\\nplexes that do not lead to net ﬂuxes of species, and for\\n10\\nthese there is no force leading to mean-regression. In the\\nlatter case, multiple steady states or non-strictly positive\\nsteady states8 cannot be ruled out [19, 20].\\nThe character of the dynamical steady states therefore\\ndepends on the relative dimension of ker A (the ﬂows that\\nabsorb or emit no material at the complexes) and ker Y A\\n(the ﬂows that absorb or emit no material at the chemi-\\ncal species), within the stoichiometric subspace. A sketch\\nof the demonstration that this is a topological character-\\nistic, following Feinberg [9, 18], Horn and Jackson [26],\\nfollows:\\nFor any CRN, the stoichiometric subspace corresponds\\nto\\nS ≡im (Y A)\\n(35)\\nThe dimension of S, and of the subspace of ﬂows through\\ncomplexes that do not produce motions within S, are\\ndenoted\\ns ≡dim (S)\\nδ ≡dim (ker (Y ) ∩im (A)) .\\n(36)\\nδ is the deﬁciency of the CRN.\\nThe following relations hold (as identities) among di-\\nmensions in Y A and A:\\ndim (im (A)) = dim (im (Y A)) + dim (ker Y ∩im (A))\\n= s + δ.\\n(37)\\nIf C is the total number of complexes, then it follows that\\nC = dim (im (A)) + dim (ker (A))\\n= s + δ + dim (ker (A))\\n(38)\\nThe expression for dim (ker (A)) is simple, and follows\\nfrom the fact that A functions as an ordinary graph\\nLaplacian on each connected component of the com-\\nplexes.\\nThe argument involves the following observa-\\ntions:\\nWeak reversibility and linkage classes: Connected\\ncomponents in the simple graph that includes only com-\\nplexes and reactions are termed linkage classes in the\\nCRN literature. Weak reversibility is the condition that\\nany node in a linkage class can be reached from any other\\nby some sequence of reactions. The subset of complexes\\nin a linkage class, which can be reached starting from\\nany complex, and which subsequently are never exited,\\nis called a strong terminal linkage class. Weak reversibil-\\nity of the whole CRN ensures that each linkage class\\nis a strong terminal linkage class, and (as in Gunawar-\\ndena [10]) we will limit to this case for simplicity.9\\n8 These would be boundary solutions where some concentrations\\nequal zero.\\n9 The more general case diﬀers only by superﬁcial book-keeping to\\nexclude complexes that are exited permanently.\\nThe counting rule for deﬁciency: Weakly reversible\\nprocesses are ergodic on each linkage class,\\nso by\\nthe Perron-Frobenius theorem or an equivalent argu-\\nment [11], there is one basis vector for ker (A) for each\\nlinkage class. Let l denote the number of linkage classes.\\nThen l = dim (ker (A)), and the Feinberg counting result\\nthat\\nδ = C −s −l\\n(39)\\nfollows from Eq. (38).\\nComplex-balanced steady states: Flows in ker (A)\\nare termed complex-balanced, because they require no net\\ntransport of ﬂux to or from any complex to the species\\nthat make it up. All steady states must (tautologically)\\nbe in ker (Y A). If δ = 0 the two spaces have the same\\ndimension and thus are the same. In other words, the\\nsteady-state condition that there be no sources or sinks\\nat species nodes entails the condition that there be no\\nsources or sinks at complexes. In this case a convexity\\nargument [9, 11] implies that there is a unique steady\\nstate in the positive orthant for any value of the rate\\nconstants.\\nNonzero deﬁciency:\\nIf δ\\n> 0 there are species-\\nbalancing ﬂows that are not complex-balancing, and\\ndim (ker (A)) is larger than the number of constraints\\nfrom ∂n/∂τ = 0 (which is only s). Steady states then\\ngenerally exist out of the subspace of ker (A).\\n1.\\nUsing mean-regression as a basis to decompose ﬂows\\nbeyond the (δ = 0)-condition\\nWe noted above that a basis for ker (A) has one vector\\non each linkage class. At the level of individual linkage\\nclasses, these are proﬁles in Ψ proportional to the max-\\nimal eigenvectors of the Perron-Frobenius theorem for\\nsimple diﬀusion under A. To characterize the remainder\\nof the space of activities on the complexes, we require a\\nbasis that decomposes the pre-image of the stoichiomet-\\nric subspace from the remaining ﬂows.\\nLet {eα}s\\nα=1 be a basis for ker (Y A)⊥⊆RC. Changes in\\nΨ along these directions lead to changes in ﬂows ∂n/∂τ\\nwithin S, which we term s-ﬂows.\\nLet {˜eβ}δ\\nβ=1 be a basis for ker (Y A) / ker (A). Changes\\nin Ψ along these directions lead to changes by species-\\nbalanced but not complex-balanced ﬂows, which do not\\nalter ∂n/∂τ and which we term δ-ﬂows.\\nIf follows that jointly\\nn\\n{eα}s\\nα=1, {˜eβ}δ\\nβ=1\\no\\nform a basis\\nfor ker (A)⊥⊆RC.\\nThis basis leads to a decomposition of A and therefore\\nof L, diﬀerent from either the reaction or the complex\\n11\\nrepresentations in Eq. (3), which we might term the “sto-\\nichiometric” representation:\\n−L = ψ†\\nY A\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\ns\\nX\\nα=1\\neαeT\\nα +\\nδ\\nX\\nβ=1\\n˜eβ˜eT\\nβ\\n\\uf8fc\\n\\uf8fd\\n\\uf8feψY\\nstoich. rep.\\n(40)\\nThe s-ﬂows and δ-ﬂows distinguished in the stoichiomet-\\nric representation turn out to make dimensionally dif-\\nferent contributions to the moment equations (34). The\\noriginal Feinberg result, expressed in this language as\\nsaying that the s-ﬂows completely determine the ﬁrst-\\nmoment equations of motion – and that these are the\\nonly such ﬂows for CRNs of deﬁciency zero – extends\\nto a claim that the s-ﬂows dominate the behavior of mo-\\nments at all orders lower than the mean particle numbers,\\nfor general CRNs where they are not the only ﬂows. We\\nexplore the consequences of these scaling dimensions in\\nSec. V.\\nB.\\nThe Feinberg deﬁciency-0 theorem and the\\nAnderson-Craciun-Kurtz theorem\\nFeinberg’s\\nDeﬁciency-0\\ntheorem\\nwas\\noriginally\\nframed [9] as a result in the convex analysis of the\\nmass-action rate law, which is generally assumed to be\\na mean-ﬁeld approximation.\\nThe Anderson-Craciun-\\nKurtz theorem [24] uses the Feinberg existence proof\\nfor solutions to the mass-action rate law, but in proving\\nthat the underlying distributions have product-Poisson\\nform,10 it actually strengthens Feinberg’s original theo-\\nrem: because factorial moments of Poisson distributions\\nhave exactly the relation to the ﬁrst moment assumed\\nin mean-ﬁeld formulations, the Feinberg solution to the\\nmass-action rate equations need not be framed as a\\nresult in (unregulated) mean-ﬁeld approximation, but\\ncan rather be seen as an exact result.\\nThe Doi operator algebra, together with the stoichio-\\nmetric decomposition in Eq. (40), provides an elegant\\nway to both prove the ACK theorem and see that it\\nmakes the Feinberg theorem exact when δ = 0, and also\\nsee why and how these results no longer hold when δ > 0.\\nWe begin with the set of cases P\\np kp = 1 of Eq. (33), in\\nwhich the sum on j contains the single term j = k; these\\nare the equations of motion for the set of ﬁrst moments.\\n10 under the sampling model assumed throughout this paper\\nFor species p:11\\n∂\\n∂τ ⟨np⟩= Yp A (0| e\\nP\\nq aqψY (a) |φ)\\n= Yp A ⟨ΨY (n)⟩\\n= YpA\\ns\\nX\\nα=1\\neαeT\\nα ⟨ΨY (n)⟩.\\n(41)\\nSo far we work at general deﬁciency, but because Y A˜eβ ≡\\n0, ∀β by construction, only the basis elements corre-\\nsponding to s-ﬂows are non-zero.\\nEq. (41) is exact for this stochastic process, and it is\\nalmost the same as the standard expression for the mass-\\naction rate law, except that it involves an expectation of\\nthe observables ΨY (n), which may include higher-order\\ncorrelations in n. Arbitrarily ignoring these correlations,\\nand replacing ⟨ΨY (n)⟩with ψY (⟨n⟩), deﬁnes the mean-\\nﬁeld approximation.\\nThe one case where the mean-ﬁeld form is exact is\\nwhen the state |φ) equals some coherent state |ξ). The\\ncoherent states are the generating functions of Poisson\\ndistributions, constructed in the Doi algebra as\\n|ξ) ≡e(a†−1)ξ |0)\\n↔e(z−1)T ξ · 1 = e−1T ξ X\\nn1\\n. . .\\nX\\nnP\\nY\\np\\nznp\\np ξnp\\np\\nnp!\\n.\\n(42)\\nξ ≡[ξp] is a vector of the mean particle numbers ξp = np\\nfor the P chemical species.\\nCoherent states are eigenstates of the Doi lowering op-\\nerator, and thus\\nψY (a) |ξ) = ψY (ξ) |ξ) .\\n(43)\\ngiving the mean-ﬁeld form as an exact result:\\n⟨ΨY (n)⟩= (0| e\\nP\\nq aqψY (a) |ξ)\\n= ψY (ξ)\\n= ψY (⟨n⟩) .\\n(44)\\nUsing the basis {eα} to handle the counting of dimen-\\nsions in the stoichiometric subspace (not all linear com-\\nbinations of np are necessarily dynamic in a particular\\nCRN), the condition that ∂n/∂τ = 0 in Eq. (41) becomes\\neT\\nαψY (ξ) = 0 ;\\n∀α ∈1, . . . , s.\\n(45)\\nThese are the set of equations proved by Feinberg to have\\na unique, strictly-positive solution when δ = 0, for all\\nnon-degenerate values of the rate constants. Thus, if the\\ndistribution |φ) is a coherent state, the Feinberg result is\\nan exact solution.\\n11 Note how, from the symmetric form of L in Eq. (27), the asym-\\nmetry of the usual rate equations has resulted from the projection\\nonto the dynamics of a particular moment.\\n12\\nThe coherent state identiﬁed by Eq. (45) is the ACK\\nsolution if it is a solution at all. However, whereas any\\nmean-ﬁeld equation can be solved for some coherent-state\\nparameter, the corresponding state is only a solution to\\nthe whole moment hierarchy if it also leads to stasis of\\nall higher moments. To see why this is assured at δ = 0\\nand not otherwise, we insert the stoichiometric decom-\\nposition (40) into the general moment hierarchy (34) to\\nobtain:\\n∂\\n∂τ\\n*Y\\np\\nn\\nkp\\np\\n+\\n=\\nk1\\nX\\nj1=0\\n\\x12\\nk1\\nj1\\n\\x13\\n. . .\\nkP\\nX\\njP =0\\n\\x12\\nkP\\njP\\n\\x13 \"\\n˙Y\\np\\nY\\njp\\np\\n#\\nA\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\ns\\nX\\nα=1\\neαeT\\nα +\\nδ\\nX\\nβ=1\\n˜eβ˜eT\\nβ\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\n\\nΨY +(k−j)(n)\\n\\x0b\\n(46)\\nOn\\na\\ncoherent\\nstate,\\nthe\\nvector\\nof\\nfactorial\\nmo-\\nments\\n\\nΨY +(k−j)(n)\\n\\x0b\\ndiﬀers from the value ⟨ΨY (n)⟩\\nfor the ﬁrst-moment condition only by an overall fac-\\ntor Q\\np ξ\\n(k−j)p\\np\\n.\\nIf δ = 0, only s-ﬂows are present\\nin\\nthe\\ndecomposition,\\nand\\nthe\\nset\\nof\\nprojections\\n\\x08\\neT\\nα\\n\\nΨY +(k−j)(n)\\n\\x0b\\t\\n= Q\\np ξ\\n(k−j)p\\np\\n×\\n\\x08\\neT\\nα ⟨ΨY (n)⟩\\n\\t\\nvan-\\nishes exactly when Eq. (45) holds, for all values of k\\nand j.\\nAlthough the truncated factorials of stoichio-\\nmetric coeﬃcients\\nh ˙\\nQ\\npY\\njp\\np\\ni\\ngenerally diﬀer from Y , this\\nchanges only the weight of the inner products with the\\neα and thus the strength with which each s-ﬂow con-\\ntributes to the rate equation away from the steady state.\\nHence the coherent-state solution identiﬁed by\\nthe ﬁrst-moment condition (45) is a steady-state\\nsolution for the whole moment hierarchy, proving\\nthe ACK theorem.\\nIf δ > 0, then δ-ﬂows also exist in the sum (46). How-\\never, whereas Y A˜eβ ≡0, ∀β,\\nh ˙Q\\npY\\njp\\np\\ni\\nwill not generally\\nproject out A˜eβ in the higher-order terms P\\np jp > 1,\\nexcept possibly in special cases of ﬁne-tuning of the rate\\nparameters.\\nTherefore, in general, k-dependent linear\\ncombinations of ˜eT\\nβ\\n\\nΨY +(k−j)(n)\\n\\x0b\\nand eT\\nα\\n\\nΨY +(k−j)(n)\\n\\x0b\\nwill be required to vanish at a steady state, obviating any\\nsimple Poisson solution.\\nV.\\nSCALING REGIMES, MATCHED\\nASYMPTOTIC EXPANSIONS, AND THE\\nCONTROLLING ROLE OF MEAN-REGRESSING\\nFLOWS\\nThe series expansion in factorials of the stoichiometry\\nfrom Eq. (34), together with the interpretation of each\\nterm as a projection operator for some product-Poisson\\ndistribution that we used to prove the ACK theorem in\\nSec. IV B, provides a way to associate diﬀerent regions in\\nthe lattice of factorial moments with control by diﬀerent\\nsubsets of the network ﬂows. Both a set of practical solu-\\ntion methods for steady states, based on matched asymp-\\ntotic expansions, and the concept of approximating a\\ncomplex distribution locally by a product-Poisson distri-\\nbution, follow when we recognize that diﬀerent terms in\\nthe series expansion are associated with diﬀerent scaling\\nbehaviors because they capture distinct combinations of\\nthe rate constants from the network.\\nThe next three sub-sections cover the following topics\\nin order:\\nDescaling: We ﬁrst introduce the descaling of the mo-\\nment hierarchy with coherent-state parameters.\\nSince,\\nfor Poisson distributions these are the only scale parame-\\nters, descaling turns the moment hierarchy for a coherent\\nstate into a vector of 1s.12\\nMatched asymptotic expansions: Generalizing the\\nFeinberg steady-state condition to all orders, as we did\\nabove to prove the ACK theorem, allows us to use the\\ncondition ΛΦ = 0 as a recursion relation on k to solve the\\nmoment hierarchy, much like the solution of any Laplace\\nequation, with Λ serving as the Laplacian on the lattice\\nof moments.\\nWhen we do this with the descaled mo-\\nment hierarchy, it becomes easy to show that recursion\\nupward in any component of k produces a convergent\\npower-series expansion on the range k/ξ ≪1 (where ξ\\nstands for whichever particle number corresponds to the\\ncomponent of k being incremented), whereas recursion\\ndownward in k is convergent for k/ξ ≫1. This suggests\\na general method of solution for moment hierarchies us-\\ning matched asymptotic expansions, where the matching\\nconditions are imposed in the region k ∼ξ.\\n1/n-expansion about Poisson backgrounds:\\nThe\\nsame small-parameter recursion that controls the asymp-\\ntotic expansion for k/ξ ≪1 also shows a sense in which\\nthe mean-regressing ﬂows (the s-ﬂows) deﬁne a leading-\\nPoisson approximation to low-order moments for a gen-\\neral CRN. This is true even when the projection onto the\\nbasis {eα} does not deﬁne a zero-deﬁciency sub-network\\nof the original CRN. We can construct a linear combina-\\ntion of the coherent-state solutions to the ﬁrst-moment\\n12 This is true as long as the descaling is done with the coherent\\nstate’s own ξ values. More generally the moment hierarchy be-\\ncomes a geometric progression.\\n13\\nsteady-state conditions for which the remainder term\\nthat must be added to obtain an exact solution makes\\na contribution that is O(k/ξ) smaller than the contribu-\\ntion of the Poisson backgrounds for the low-order mo-\\nments (those with k/ξ ≪1).\\nA.\\nDescaling the moment equations with\\ncoherent-state parameters\\nJust as Yp projects out the ACK product-Poisson for\\nδ = 0 networks, each of the projectors\\nh ˙Q\\npY\\njp\\np\\ni\\nin\\nthe moment-recursion equation (34) projects out some\\nproduct-Poisson distribution if values for the correspond-\\ning coherent-state parameters can be found. (If they are\\nnot unique, it can project out more than one such so-\\nlution.) We may choose to reference exact solutions for\\nΦk to locally-chosen Poisson distributions in diﬀerent re-\\ngions of k corresponding to diﬀerent terms j ≡[jp], and\\nlet the recursion equations solve for the (smaller) devia-\\ntions from these reference-Poissons.\\nFor any vector ξ ≡[ξp] of mean values, we may descale\\nthe activities Ψi\\nY from Eq. (23) as\\nˆΨi\\nY (n) ≡\\nY\\np\\nn\\nyi\\np\\np\\nξ\\nyip\\np\\n(47)\\nIf we normalize the ΨY vectors in this way, a correspond-\\ning counter-normalization of the adjacency matrix can be\\ndeﬁned as\\nˆAji ≡Aji\\nY\\np\\nξ\\nyi\\np\\np .\\n(48)\\nIn cases where we wish to use the stoichiometric repre-\\nsentation (40), a similar descaling of the corresponding\\nprojection vectors is\\n(ˆeα)i ≡\\n1\\nQ\\np ξ\\nyip\\np\\n(eα)i\\n\\x00ˆeT\\nα\\n\\x01\\ni ≡\\n\\x00eT\\nα\\n\\x01\\ni\\nY\\np\\nξ\\nyi\\np\\np\\n\\x10\\nˆ˜eβ\\n\\x11\\ni ≡\\n1\\nQ\\np ξ\\nyip\\np\\n(˜eβ)i\\n\\x10\\nˆ˜e\\nT\\nβ\\n\\x11\\ni ≡\\n\\x00˜eT\\nβ\\n\\x01\\ni\\nY\\np\\nξ\\nyi\\np\\np ,\\n(49)\\nfor all α and β. All these are chosen so that\\nA ⟨ΨY (n)⟩≡ˆA\\nD\\nˆΨY (n)\\nE\\n≡ˆA\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\ns\\nX\\nα=1\\nˆeαˆeT\\nα +\\nδ\\nX\\nβ=1\\nˆ˜eβˆ˜e\\nT\\nβ\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\nD\\nˆΨY (n)\\nE\\n(50)\\nApplying the scale transformations (47,48) to the equa-\\ntions of motion (34) for the moment hierarchy Φ gives the\\nequation for a descaled hierarchy ˆΦ, in which the leading\\ngeometric dependence on ξ has been factored out:\\n∂\\n∂τ\\nˆΦk ≡∂\\n∂τ\\n*Y\\np\\nn\\nkp\\np\\nξkp\\np\\n+\\n=\\nk1\\nX\\nj1=0\\n\\x12\\nk1\\nj1\\n\\x13\\n. . .\\nkP\\nX\\njP =0\\n\\x12\\nkP\\njP\\n\\x13 \"\\n˙Y\\np\\nY\\njp\\np\\nξjp\\np\\n#\\nˆA\\nD\\nˆΨY +(k−j)(n)\\nE\\n≡\\nX\\nk′\\nˆΛkk′ ˆΦk′.\\n(51)\\nFollowing Feinberg but extending his consideration to all\\nmoments, we try to construct steady states, for which\\nEq. (51) can be used as a recursion relation among the\\nmoments of ˆΦk.\\nB.\\nMatched asymptotic expansions for the\\nsteady-state condition\\nThe terms that govern the behavior of recursions in\\nthe components of k, if Eq. (51) is used to (exactly or\\napproximately) solve for ˆΦk, are combinations of the form\\n\\x12\\nk\\nj\\n\\x13 Y j\\nξj =\\nk!\\n(k −j)! ξj\\nY j\\nj! .\\n(52)\\n(with k, j, Y , and ξ carrying indices for each p, which we\\nsuppress to reduce clutter). The ratios Y j/j! are ﬁxed\\nparameters of ˆΛ and in any case only ﬁnite in number.\\nThe ratios that govern scaling behavior across the mo-\\nment hierarchy are the terms k!/\\n\\x02\\n(k −j)! ξj\\x03\\n. Because\\nthe stoichiometric coeﬃcients and therefore the limits in\\nthe sums over j are ﬁnite, it is possible to consider a\\nrange of typical particle number n ∼ξ ≫max (j), in\\nwhich k!/\\n\\x02\\n(k −j)! ξj\\x03\\n∼(k/ξ)j in the ranges that gov-\\nern the transition between scaling regions. These scale\\nfactors govern the stability of asymptotic expansions as\\nfollows:\\nFor any ﬁxed value of k, increasing j in the sum (51)\\nlowers the order of all moments in\\nD\\nˆΨY +(k−j)(n)\\nE\\n, at\\nthe same time multiplying the corresponding term by a\\ncoeﬃcient ∼(k/ξ)j. Let j be the smallest value at which\\n14\\n˙Q\\np\\n\\x10\\nY\\njp\\np /ξjp\\np\\n\\x11\\nˆA does not vanish. (In general, this occurs\\nwhen P\\nq jq = 1, so exactly one of the terms Y\\njp\\np\\n= Yp\\nand Y\\njq\\nq\\n= 1 for all other q ̸= p.)\\nTo extend the recursion upward by one order, we must\\nincrement k while holding j ﬁxed at j. The new moments\\nappearing at order k are referred to those at the imme-\\ndiately preceding order in the recursion by higher-order\\nterms j > j in the sum at the current k. The relative\\nmagnitude of the preceding terms to the new terms scales\\nas ∼(k/ξ)j−j.\\nFor k ≪ξ, successively higher-order\\nterms are expressed as sums of lower-order terms with\\npositive powers of (k/ξ), consistent with both a non-zero\\nradius of convergence, and with damping-out of uncer-\\ntainties in the initial conditions of the recursion. (The\\nlatter property is important for an asymptotic expansion\\nto provide a robust solution algorithm.)\\nFor k ≫ξ the opposite is true: the lower-order terms\\nmust be solved as functions of the higher-order terms,\\nwhich are multiplied by positive powers of (ξ/k). Thus\\nin this range the downward recursion is consistent with\\na non-zero radius of convergence, and damps out uncer-\\ntainties in the starting conditions assumed at large k.\\nThis argument is the basis for a solution in terms of\\nmatched asymptotic expansions, where stable recursions\\nare carried out starting respectively from k = 0 (up-\\ngoing) and from asymptotically large k (down-going),\\nand matching conditions are imposed in the overlap re-\\ngion k ∼ξ, which are marginally stable for both series.\\nAn interesting feature of this solution is that, for CRNs\\nwhere the mean-ﬁeld approximation predicts multiple\\nsteady states, there may still be unique large-k asymp-\\ntotic behaviors required to ensure boundedness of mo-\\nments at all orders. In such cases, it is the downward re-\\ncursion from large k that anchors the solution to the mo-\\nment hierarchy. This is a counter-intuitive result given\\nthe conventional mean-ﬁeld approach to moment closure,\\nwhich attempts to anchor all higher-order moments in so-\\nlutions to the ﬁrst moments, but as a consequence cannot\\nobtain the ergodic sum over multiple steady states which\\nis a property of the exact all-orders solution.\\nWe do not oﬀer a formal proof that these asymptotic\\nexpansions can be consistently performed for all CRNs\\nand all dimensionalities of the moment hierarchy, which\\nis an exercise beyond the scope of the current paper.\\nHowever, in one dimension, the recursion is elementary\\nto deﬁne, and for higher-dimensional systems we oﬀer\\nexamples of decompositions of the solution for which nu-\\nmerical simulation suggests that a similar expansion can\\nbe used.\\nC.\\nLeading Poisson approximations to\\nnonzero-deﬁciency CRNs\\nThe above analysis of the scaling of terms in an asymp-\\ntotic expansion for solutions to ˆΛˆΦ = 0 has an immedi-\\nate corollary: in general steady-state solutions, a basis\\nof product-Poisson distributions associated with s-ﬂows\\ndominates the low-order moments. The kernel of the ar-\\ngument is that, although the projection operators asso-\\nciated with both s- and δ-ﬂows share in the same scal-\\ning, at the lowest order where the upward-going recursion\\nbegins, the δ-ﬂow contributions are projected out – re-\\ncall that their absence from the ﬁrst-moment conditions\\nis their deﬁning feature – therefore only s-ﬂow contri-\\nbutions serve as seeds for the polynomial expansion in\\n(k/ξ). We now demonstrate that relation:\\nThe steady-state condition for Eq. (51), with the sto-\\nichiometric decomposition inserted from Eq. (40), be-\\ncomes\\n0 =\\nk1\\nX\\nj1=0\\n\\x12\\nk1\\nj1\\n\\x13\\n. . .\\nkP\\nX\\njP =0\\n\\x12\\nkP\\njP\\n\\x13 \"\\n˙Y\\np\\nY\\njp\\np\\nξjp\\np\\n#\\nˆA\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\ns\\nX\\nα=1\\nˆeαˆeT\\nα +\\nδ\\nX\\nβ=1\\nˆ˜eβˆ˜e\\nT\\nβ\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\nD\\nˆΨY +(k−j)(n)\\nE\\n(53)\\nThe lowest-k conditions that the moment hierarchy must\\nsatisfy are the ﬁrst-moment conditions, which are the set\\nof terms P\\nq kq = 1 and j = k, for which exactly one\\nY\\njp\\np\\n= Yp and Y\\njq\\nq\\n= 1 for all other q ̸= p, as noted in the\\nprevious section.\\nWrite the state vector |φ) for a general solution to\\nEq. (53) as a sum\\n|φ) =\\nX\\nγ\\ncγ\\n\\x0c\\x0c\\x0cξ(γ)\\x11\\n+ |φ′) ,\\n(54)\\nin which\\n\\x08\\nξ(γ)\\t\\nis the set of all mean-ﬁeld solutions\\nto the ﬁrst-moment steady state conditions, and cγ\\nare coeﬃcients to be determined.\\nBy construction\\neT\\nα\\nP\\nγ cγψY\\n\\x00ξ(γ)\\x01\\n= 0, ∀α.13 Refer to the corresponding\\n13 Note that |φ′), as the generating function for a diﬀerence of dis-\\ntributions, will not generally be derived from any distribution\\nwith all positive values.\\n15\\nexpectations as\\n(0| e\\nP\\nq aqψY (a) cγ\\n\\x0c\\x0c\\x0cξ(γ)\\x11\\n≡⟨ΨY (n)⟩(γ)\\n(0| e\\nP\\nq aqψY (a) |φ′) ≡⟨ΨY (n)⟩′,\\n(55)\\nand likewise for higher-order moments and descaled mo-\\nments ˆΨ.\\nUnder the decomposition (54), the steady-state condi-\\ntion (53) becomes\\nk1\\nX\\nj1=0\\n\\x12\\nk1\\nj1\\n\\x13\\n. . .\\nkP\\nX\\njP =0\\n\\x12\\nkP\\njP\\n\\x13 \"\\n˙Y\\np\\nY\\njp\\np\\nξjp\\np\\n#\\nˆA\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\ns\\nX\\nα=1\\nˆeαˆeT\\nα +\\nδ\\nX\\nβ=1\\nˆ˜eβˆ˜e\\nT\\nβ\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\nD\\nˆΨY +(k−j)(n)\\nE′\\n= −\\nX\\nγ\\nk1\\nX\\nj1=0\\n\\x12\\nk1\\nj1\\n\\x13\\n. . .\\nkP\\nX\\njP =0\\n\\x12\\nkP\\njP\\n\\x13 \"\\n˙Y\\np\\nY\\njp\\np\\nξjp\\np\\n#\\nˆA\\nδ\\nX\\nβ=1\\nˆ˜eβˆ˜e\\nT\\nβ\\nD\\nˆΨY +(k−j)(n)\\nE(γ)\\n.\\n(56)\\nIf there is a unique steady-state solution, and if the\\ndescaled moment hierarchy\\nˆΦ is descaled with this\\nξ,\\nthen by construction it will be the case that\\nD\\nˆΨY +(k−j)(n)\\nE(γ)\\n= cγ [1] (the vector of all 1s), ∀k, j\\non the right-hand side of Eq. (56), which contains a\\nsingle term in the sum on γ.14\\nIn the more general\\ncase, we can choose the descaling parameters so that\\nD\\nˆΨY +(k−j)(n)\\nE(γ)\\n= cγ [1] for a particular γ of our\\nchoice. (Generally this means descaling whichever term\\nmakes the largest contribution on the right-hand side, to\\nremove k- and j- dependence in that term.) The argu-\\nment that the term |φ′) is sub-leading is then made in\\ntwo steps:\\n1. For some set of coeﬃcients {cγ} we can ensure that\\neT\\nα⟨ΨY (n)⟩′ = 0. This is because the set of steady-\\nstate solutions for ξ(γ) form a basis for the set of\\nall solutions to the ﬁrst-moment steady-state condi-\\ntions. In general (even if there is only one solution\\nfor ξ), the required coeﬃcients {cγ} may need to\\nbe determined by matching conditions to a large-k\\nasymptotic expansion.15 (Note that the values of\\n˜eT\\nβ ⟨ΨY (n)⟩′ are unconstrained at order P\\np kp = 1,\\nand must be determined as part of the recursion on\\nk.)\\n2. For P\\np kp\\n>\\n1, the leading-order dependence\\non\\nD\\nˆΨY +(k−j)(n)\\nE(γ)\\non the right-hand side of\\n14 Note that, even in the case of a unique steady state, we cannot\\npresume that cγ = 1 unless ⟨ΨY ⟩includes a term proportional to\\nΦ0 ≡⟨1⟩, because the ﬁrst-moment condition does not otherwise\\nﬁx the normalization of the geometric sequence within the total\\ndistribution.\\n15 The mono-stable solution in Sec. VI B illustrates the need for a\\nnon-trivial normalization in the case of a unique solution, and\\nthe bistable solution in Sec. VI C illustrates the case of solution\\nfor a linear combination of Poisson backgrounds.\\nEq. (56) comes when P\\np jp = 2, and by Eq.(52)\\nthis term is O\\n\\x14\\n(k/ξ)2D\\nˆΨY +(k−j)(n)\\nE(γ)\\x15\\n.\\nThe\\nleading term on the left-hand side arises where\\nP\\np jp\\n=\\n1, involves only the s-ﬂows, and is\\nO\\nh\\n(k/ξ)\\n\\nΨY +(k−j)(n)\\n\\x0b′i\\n. Hence we conclude that,\\nat the level of counting naive scaling dimen-\\nsions, there is a perturbative expansion in small\\n(k/ξ) about a sum of coherent states, in which\\n\\nΨY +(k−j)(n)\\n\\x0b′\\n∼\\nO\\n\\x14\\n(k/ξ)\\nD\\nˆΨY +(k−j)(n)\\nE(γ)\\x15\\n.\\nThe reason it is meaningful to make such a scaling\\ncomparison, when the j values used to estimate the\\npowers of scale factors are diﬀerent on the left-hand\\nand right-hand sides of Eq. (56), is that we have\\nbeen free to choose the de-scaling parameter for ˆΦ\\nso that for whichever γ gives the largest contribu-\\ntion,\\nD\\nˆΨY +(k−j)(n)\\nE(γ)\\n= cγ [1], removing k- and j-\\ndependence from the source on the right-hand side\\nof the equation.\\nThis completes the argument.\\nVI.\\nWORKED EXAMPLES\\nWe now demonstrate the above results for moment hi-\\nerarchies in a cascade of examples. Each successive ex-\\nample increases the generality of the problem and in-\\ntroduces a new feature of general solutions of CRN’s.\\nExplicit constructions involving transfer matrices or Li-\\nouville operators are given in the main text where they\\nﬁrst occur, and the corresponding forms that diﬀer only\\nby elaboration for later examples are removed to App. A.\\n16\\nA.\\nA CRN with 1-species, 2 states and no\\nconserved quantities\\nThis model is a minimal non-trivial form for a CRN,\\nshowing how uniqueness of positive steady states follows\\nfrom deﬁciency-0, and exhibiting the proof of the ACK\\ntheorem in terms of coherent-state projection operators\\nfrom Sec. IV B. The CRN is given by the graph intro-\\nduced in Fig. 1 with the associated reaction scheme (1).\\nIts mean-ﬁeld rate equation is\\n∂n\\n∂τ = αn −βn2.\\n(57)\\nThe master equation, illustrating the decomposition (26)\\nfor the transfer matrix, is\\n∂ρn\\n∂τ =\\nh\\x10\\ne−∂/∂n −1\\n\\x11\\nαn +\\n\\x10\\ne∂/∂n −1\\n\\x11\\nβn (n −1)\\ni\\nρn.\\n(58)\\nFor the transfer matrix (58), the Liouville operator is\\nL =\\n\\x001 −a†\\x01 \\x00αa†a −βa†a2\\x01\\n=\\n\\x001 −a†\\x01 \\x00a†a\\n\\x01\\n(α −βa) .\\n(59)\\nThe ﬁrst line is a direct translation of the reaction-\\nrepresentation from Eq. (3) for the conversion of particles\\nin each unidirectional reaction. The second line extracts\\nthe overall factor of the projection operator (α −βa) that\\nvanishes on a coherent state with parameter ξ = α/β in\\nall moment equations, which is the proof of the ACK\\ntheorem given in Sec. IV B.\\nB.\\nA CRN with 1-species, 3 states and no\\nconserved quantities\\nIn this section we introduce a one-parameter family of\\nmodels which have the same rate equation over the en-\\ntire family. Fig. 2 depicts a limiting member which lacks\\nweak reversibility. Although the limiting case is formally\\noutside the scope of the assumptions in the rest of the\\npaper, it is useful to highlight the role of δ-ﬂows in driv-\\ning solutions away from the Poisson form associated with\\nthe ACK theorem. Weak reversibility may be established\\nwithout changing the rate equation by adding two reac-\\ntions to the graph of Fig. 2, to obtain the family (over\\nthe rate parameter ϵ) of graphs shown in Fig. 3, which\\nwe analyze below.\\nFor this model, we demonstrate the stoichiometric de-\\ncomposition of the Liouville operator from Eq. (40), and\\ndescaling of the moment recursion equation. For the pa-\\nrameters we will use in simulations, a single, downward-\\ngoing asymptotic expansion (obtained in [33]) is suﬃ-\\ncient to solve the entire moment hierarchy to arbitrary\\nprecision, starting from an analytically derived large-k\\nlimiting form.\\nHowever, we will also demonstrate the\\nupward/downward matched asymptotic expansion to il-\\nlustrate the stability properties of the recursion in small-\\nk and large-k ranges. Exact solutions to moment hier-\\narchies of this kind are only possible for birth-death [34]\\ntype CRNS with one species, of which this class of models\\nis an example, or for δ = 0 CRN’s, whereas the asymp-\\ntotic expansions have a much wider applicability, as we\\ndemonstrate in later sections.\\nThe rate equation for the CRN’s in both Fig.\\n2 and\\nFig. 3 is\\n∂n\\n∂τ = αn −βn3.\\n(60)\\nwhich diﬀers from Eq. (57) only in changing the activities\\nthat govern particle creation and destruction.\\nα\\nβ\\nFIG. 2:\\nAn additional state added relative to the model of\\nFig. 1. This model is not weakly reversible.\\nThe simplest CRN graph with rate equation (60) is\\nshown in Fig. 2, with associated reaction scheme\\nA\\nα⇀2A\\n2A\\nβ↽3A.\\n(61)\\nThe model introduces competing auto-catalysis at two\\norders: particle creation occurs in proportion to the den-\\nsity of existing particles, while particle destruction occurs\\nin proportion to the cube of the density.\\nBecause the\\nCRN in Fig. 2 has only three complexes, however, the\\nrate equation has only two non-trivial roots, and there-\\nfore cannot support multiple positive steady states. (It\\ndoes, however, have the marginally stable steady state\\nn ≡0.)\\nε\\nα\\nβ\\nε\\nFIG. 3:\\nA variant one-species model in which the complex-\\ngraph is weakly reversible. The steady-state concentrations\\nare the same as those from Fig. 2, which appears as a regular\\nlimit at ϵ →0. Because δ = 1, the distribution at the steady\\nstate is no longer Poisson.\\nThe CRN of Fig. 3 with reaction scheme\\nA\\nα−⇀\\n↽−\\nϵ 2A\\n2A\\nϵ−⇀\\n↽−\\nβ 3A.\\n(62)\\nThese may be checked from Eq. (39) to have deﬁciency\\nδ = 1.\\nThe master equation is provided in Eq. (A2), and the\\nassociated Liouville operator is\\nL =\\n\\x001 −a†\\x01 h\\nαa†a −ϵ\\n\\x001 −a†\\x01\\na†a2 −βa†2a3i\\n=\\n\\x001 −a†\\x01 \\x00a†a\\n\\x01 \\x02\\n(α −ϵa) +\\n\\x00a†a −1\\n\\x01\\n(ϵ −βa)\\n\\x03\\n.\\n(63)\\n17\\nThe limit ϵ →0 is degenerate with the graph of Fig. 2.\\nThe important feature of this Liouville operator is that\\nthe two projection terms, (α −ϵa) and (ϵ −βa), are now\\nmultiplied by distinct non-trivial operators (respectively\\n1 and\\n\\x00a†a −1\\n\\x01\\n, and cannot both be made to vanish in-\\ndependently at a single Poisson solution at general values\\nof ϵ. This is the way in which deﬁciency-1 (attained by\\nadding a complex within a linkage class) moves the CRN\\noutside the scope of the ACK theorem.\\n1.\\nThe stoichiometric decomposition\\nWe can use the property suggested by the concept\\nof deﬁciency – the categorization of ﬂows as mean-\\nregressing versus non-mean-regressing – to further clarify\\nhow the non-independence of the projection terms in the\\nLiouville operator (63) results in deviations from Poisson\\nsteady-state form.\\nThe three decompositions of the Liouville opera-\\ntor (63), from Eq. (3) and Eq. (40), are given by\\nL =\\nh\\na† a†2 a†3 i \\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\uf8ee\\n\\uf8f0\\n1\\n−1\\n0\\n\\uf8f9\\n\\uf8fb\\n\\x02 α −ϵ 0 \\x03\\n+\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n−1\\n\\uf8f9\\n\\uf8fb\\n\\x02 0 ϵ −β \\x03 \\uf8fc\\n\\uf8fd\\n\\uf8fe\\n\\uf8ee\\n\\uf8f0\\na1\\na2\\na3\\n\\uf8f9\\n\\uf8fb\\n=\\nh\\na† a†2 a†3 i \\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\uf8ee\\n\\uf8f0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fb\\n\\x02 α −ϵ 0 \\x03\\n+\\n\\uf8ee\\n\\uf8f0\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fb\\n\\x02 −α 2ϵ −β \\x03\\n+\\n\\uf8ee\\n\\uf8f0\\n0\\n0\\n1\\n\\uf8f9\\n\\uf8fb\\n\\x02 0 −ϵ β \\x03 \\uf8fc\\n\\uf8fd\\n\\uf8fe\\n\\uf8ee\\n\\uf8f0\\na1\\na2\\na3\\n\\uf8f9\\n\\uf8fb\\n=\\nh\\na† a†2 a†3 i\\n1\\nα2 + β2\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n\\uf8ee\\n\\uf8f0\\nα2\\n−α2 + β2\\n−β2\\n\\uf8f9\\n\\uf8fb\\n\\x02 α 0 −β \\x03\\n+\\n\\uf8ee\\n\\uf8f0\\n1\\n−2\\n1\\n\\uf8f9\\n\\uf8fb\\n\\x02\\nαβ2 −ϵ\\n\\x00α2 + β2\\x01\\nβα2 \\x03 \\uf8fc\\n\\uf8fd\\n\\uf8fe\\n\\uf8ee\\n\\uf8f0\\na1\\na2\\na3\\n\\uf8f9\\n\\uf8fb\\n(64)\\nThe top line is the reaction representation (since the reac-\\ntions are bi-directional, we have combined both departure\\nterms in the row vectors). The middle line is the com-\\nplex representation. In this representation it is clear why,\\nif ϵ →0, the reaction cannot be complex-balanced: all\\nterms in a given row vector have the same sign, so any\\npositive density produces non-zero ﬂows at some com-\\nplexes. The bottom line is the stoichiometric represen-\\ntation. The ﬁrst dyadic corresponds to the s-ﬂow AeeT ,\\nand the second dyadic corresponds to the δ-ﬂow A˜e˜eT .\\nNote that the diagonalization of the s-ﬂow couples ac-\\ntivity in the complex a to changes of probability across\\ncomplexes a†2 and a†3, and vice-versa with activity at a3\\nand change of probability across complexes a† and a†2.\\nThus, despite the similarity in form to the deﬁciency-0\\nprojector in the Liouville operator from Eq. (59), the s-\\nﬂow projection in Eq. (64) cannot be written as a stand-\\nalone Liouville operator from a deﬁciency-0 sub-network\\nof the current network.\\nTo illustrate the way in which diﬀerent combinations\\nof s- and δ-ﬂows control the scaling of Φk in diﬀerent\\nregions, we note the forms of projection operators at dif-\\nferent orders in the sum (34):\\nY = Y 1 =\\n\\x02 1 2 3 \\x03\\nY 1A =\\n\\x02 α 0 −β \\x03\\nY 2 =\\n\\x02 0 2 6 \\x03\\nY 2A\\n2!\\n=\\n\\x02 α ϵ −2β \\x03\\nY 3 =\\n\\x02 0 0 6 \\x03\\nY 3A\\n6!\\n=\\n\\x02 0 ϵ −β \\x03\\n(65)\\nThe lowest-order term Y 1A projects out solutions\\nΦk+2/Φk = α/β, while the highest-order term Y 3A\\nprojects out the solution Φk+2/Φk+1 = ϵ/β. These turn\\nout to be the two limiting moment ratios, respectively, in\\nthe limits k = 1 (the moment recursion formula has no\\nterm at k = 0) and k →∞, as we now demonstrate.\\n2.\\nScaling behavior of the rate equation used as a recursion\\nrelation\\nThe Poisson background in the expansion (54), pro-\\njected out by Y 1A and ensuring vanishing of the s-ﬂow\\ncontribution to the moment dynamics at each order k, is\\ngiven by ψY =\\n\\x02 ξ ξ2 ξ3 \\x03T , where the mean number ξ\\n18\\nsatisﬁes\\nξ2 = α\\nβ .\\n(66)\\nThis CRN has a unique steady state, so the terms ap-\\npearing in Eq. (56) are:\\nk Y\\nξ\\nˆAˆeˆeT D\\nˆΨY +(k−1)(n)\\nE′\\n= αk\\n\\x10\\nˆΦ′\\nk −ˆΦ′\\nk+2\\n\\x11\\nk\\nX\\nj=2\\n\\x12\\nk\\nj\\n\\x13 Y j\\nξj ˆA\\nn\\nˆeˆeT + ˆ˜eˆ˜e\\nT o D\\nˆΨY +(k−j)(n)\\nE′\\n= α\\nξ\\nk!\\n(k −2)!\\n\\x1a\\x10\\nˆΦ′\\nk−1 −ˆΦ′\\nk+1\\n\\x11\\n+\\n\\x12\\nϵ\\n√αβ\\nˆΦ′\\nk −ˆΦ′\\nk+1\\n\\x13\\n+\\n\\x12k −2\\nξ\\n\\x13 \\x12\\nϵ\\n√αβ\\nˆΦ′\\nk−1 −ˆΦ′\\nk\\n\\x13\\x1b\\nc0\\nk\\nX\\nj=2\\n\\x12\\nk\\nj\\n\\x13 Y j\\nξj ˆAˆ˜eˆ˜e\\nT ˆψY (ξ) = c0\\nα\\nξ\\nk!\\n(k −2)!\\n\\x12\\n1 + k −2\\nξ\\n\\x13 \\x12\\nϵ\\n√αβ −1\\n\\x13\\n(67)\\nAs the numerical evaluations below will show, this is a\\nmodel in which, despite uniqueness of the Poisson solu-\\ntion matching the ﬁrst and third moments, the overall\\nnormalization of the moment hierarchy is not anchored\\nin the lowest term Φ0, so a relative normalization c0 for\\nthe ψY (ξ) contribution is undetermined. In this way, the\\nrole of the Poisson background in an exact solution of\\nthe moment hierarchy is diﬀerent from a mean-ﬁeld ap-\\nproximation. MFT would require ⟨n⟩2 = α/β in place\\nof Eq. (66), which requires ⟨n⟩2 = (c0ξ)2. The freedom\\nfor ξ to diﬀer from ⟨n⟩by the normalization c0 is nec-\\nessary, because even for this simple network, the MFT\\nprediction for the mean is not valid.\\nThe exact recursion relation for the deviations from\\nPoisson moments is then the ﬁnite sum\\nˆΦ′\\nk −ˆΦ′\\nk+2 + k −1\\nξ\\n\\x1a\\x10\\nˆΦ′\\nk−1 −ˆΦ′\\nk+1\\n\\x11\\n+\\n\\x12\\nϵ\\n√αβ\\nˆΦ′\\nk −ˆΦ′\\nk+1\\n\\x13\\n+\\n\\x12k −2\\nξ\\n\\x13 \\x12\\nϵ\\n√αβ\\nˆΦ′\\nk−1 −ˆΦ′\\nk\\n\\x13\\x1b\\n= −c0\\nk −1\\nξ\\n\\x12\\n1 + k −2\\nξ\\n\\x13 \\x12\\nϵ\\n√αβ −1\\n\\x13\\n(68)\\nFor parameters that produce suitably small mean par-\\nticle number, the recursion relation implied by Eq. (68)\\nmay be solved for all k to any desired precision, from the\\nupper asymptotic behavior alone, as shown in [33]. We\\nare interested here, however, in understanding the small-\\nk and large-k behavior of the moments. To this eﬀect,\\nwe can see directly both aspects of the scaling presented\\nin Sec. V. As an asymptotic expansion, the recursion re-\\nlation speciﬁes the higher-order diﬀerence\\n\\x10\\nˆΦk+2 −ˆΦk\\n\\x11\\nas a power series in (k/ξ) with coeﬃcients from values\\nand diﬀerences of Φ at lower k indices.\\nThe seed for the expansion at orders ˆΦ′\\n4 and higher\\nfrom the Poisson background is the k-independent value\\nc0\\n\\x00ϵ/√αβ −1\\n\\x01\\n, multiplied by a polynomial of O(k/ξ).\\nNote, however, that the terms ˆΦ′\\n2 and c0, which are unde-\\ntermined by the ﬁrst-moment condition eT ⟨ΨY (n)⟩= 0,\\nenter the recursion relation according to the scaling of\\nthe overall asymptotic expansion, and are permitted to\\nbe O(1) relative to\\n\\x00ϵ/√αβ −1\\n\\x01\\n.\\nFinally, we observe that for a ﬁne-tuned value of the\\nrate parameters ϵ = √αβ, the correction term can be\\nmade to vanish, and the ACK-like solution projected\\nto zero by the s-ﬂow term AeeT in Eq. (64) becomes\\na steady-state solution, even though for this CRN δ = 1.\\nNext we illustrate how the asymptotic expansion\\nwith a requirement of boundedness at large k anchors\\nthe moment hierarchy at all orders.\\nWe seed the\\ndownward-going asymptotic expansion with the lead-\\ning non-constant approximation to the recursion rela-\\ntion around the limiting ratio projected out by Y 3A from\\nEq. (65), which has the form\\n\\nnk+1\\x0b\\n⟨nk⟩\\n≈\\n\\x12 ϵ\\nβ\\n\\x13 \\x14\\n1 −ϵ2 −αβ\\nϵβ\\n1\\nk −1\\n\\x15\\n.\\n(69)\\nThe\\ncorresponding\\nleading-order\\napproximation\\nfor\\nlarge-k moments may be written about any reference\\nvalue k0 as\\n\\nnk\\x0b\\n≈N\\n\\x12 ϵ\\nβ\\n\\x13k \\x14\\n1 −ϵ2 −αβ\\nϵβ\\nlog\\n\\x12k −1\\nk0\\n\\x13\\x15\\n≈N\\n\\x12 ϵ\\nβ\\n\\x13k\\x12 k0\\nk −1\\n\\x13(ϵ2−αβ)/(ϵβ)\\n,\\n(70)\\nwhere N is an overall normalization to be determined.\\nFig. 4 shows a comparison of the numerical regres-\\nsion from Eq. (68), both for an upward-going recursion\\nfrom k = 1, and for a downward-going recursion with the\\nlarge-k asymptotic seed (70), to estimates of the ﬁrst 10\\nmoments from a Gillespie simulation of the underlying\\nprocess. The parameters used in the demonstration are:\\nα = 100, β = 10; ϵ = 70. So the relevant parameters are\\n19\\nξ =\\np\\nα/β =\\n√\\n10; ϵ/β = 7; ϵ/√αβ = ϵ/βξ ≈2.2136.\\nWe have normalized the constant c0, which is unspeciﬁed\\nby the recursion relation to the simulated mean ⟨n⟩, and\\nﬁnd close agreement to all other the moment ratios.\\nC.\\nA CRN with 1-species, 4 states, 2 linkage\\nclasses and no conserved quantities\\nWe can preserve the number of s-ﬂows and δ-ﬂows\\nfrom the previous model, but introduce the possibility\\nfor multi-stability, by increasing both the number of com-\\nplexes and the number of linkage classes by one. The re-\\nsulting graph for a minimal model with this elaboration\\nis shown in Fig. 5, and its reaction scheme is given by\\n∅\\nϵ\\n−⇀\\n↽−\\nk2\\nA\\n2A\\nk1\\n−⇀\\n↽−\\n¯k1\\n3A.\\n(71)\\nThe corresponding rate equation is\\n∂n\\n∂τ = ϵ −k2n + k1n2 −¯k1n3.\\n(72)\\nWe have labeled the reaction rate constants in this model\\nto reﬂect a set of cases that are often important in bio-\\nchemistry and industrial synthesis: a reaction that is (di-\\nrectly or indirectly) self-catalyzed by feedback through\\nthe synthetic network is the main channel for produc-\\ntion and decay of the product (rate constants k1 and ¯k1),\\ncompared to an uncatalyzed pathway that has nonzero\\nbut small rate (ϵ), while a signiﬁcant rate (k2) remains\\nfor spontaneous decay of the product.\\nQuadratic-order autocatalysis in this CRN comes from\\nthe same pair of reactions as it does in Fig. 3. The ad-\\ndition of a fourth state to the complex graph creates a\\ncubic ﬁrst-moment rate equation and thus the possibility\\nfor multiple steady states.\\nFor the graph of Fig. 5, the Liouville operator is\\nL =\\n\\x001 −a†\\x01 h\\nϵ −k2a + k1a†2a2 −¯k1a†2a3i\\n=\\n\\x001 −a†\\x01 \\x02\\n(ϵ −k2a) +\\n\\x00a†a\\n\\x01 \\x00a†a −1\\n\\x01 \\x00k1 −¯k1a\\n\\x01\\x03\\n.\\n(73)\\nThe stoichiometric decomposition of this operator is sim-\\nilar to that from the previous model, and is given in\\nEq. (A5).\\nThe factorials Y j and projection operators Y jA ap-\\npearing in Eq. (34) are given by\\nY = Y 1 =\\n\\x02 0 1 2 3 \\x03\\nY 1A =\\n\\x02\\nϵ −k2 k1 −¯k1\\n\\x03\\nY 2 =\\n\\x02 0 0 2 6 \\x03\\nY 2A\\n2!\\n= 2\\n\\x02\\n0 0 k1 −¯k1\\n\\x03\\nY 3 =\\n\\x02 0 0 0 6 \\x03\\nY 3A\\n3!\\n=\\n\\x02\\n0 0 k1 −¯k1\\n\\x03\\n(74)\\nThe lowest-order (in k/ξ) projector in Eq. (74) is Y 1A,\\nwhich is the projection operator corresponding to the s-\\nﬂow in Eq. (A5) in the appendix. If required to vanish\\non a coherent state, it gives\\nϵ −k2ξ + k1ξ2 −¯k1ξ3 = 0\\n(75)\\nFor appropriate parameter choices, this may have either a\\nunique stable solution, or it may have three solutions, two\\nstable and one between them that is unstable. Unlike the\\nmodel of Fig. 3, both Φ0 and Φ2 have non-zero coeﬃcients\\nin ⟨ΨY (n)⟩, so the normalization of the mean is ﬁxed\\nrelative to Φ0 ≡1. Solutions of the form (54) must satisfy\\nP\\nγ cγ = 1, and therefore we may set ⟨ΨY (n)⟩′ ≡0.\\nIn this model (contrasted with the result in Eq. (65)),\\nboth of the projection operators Y 2A and Y 3A cancel\\nthe same ratio Φk+2/Φk+1 = k1/¯k1 ≡K1, so there are\\nonly two scaling behaviors expressed in the model, re-\\nspectively at k →0 and k →∞.\\nThis CRN, also being a birth-death type process, can\\nbe solved exactly [40] for the steady state. As for the 3-\\nstate model, this moment hierarchy may also be solved by\\nrecursion from an upper asymptotic limit that is deriv-\\nable analytically (though again the numerical calculation\\nis stable only for suﬃciently small mean particle num-\\nbers.) If we deﬁne\\n\\nnk\\x0b\\n/\\n\\nnk−1\\x0b\\n≡Rk, then the set of\\nRk must obey the recursion\\nRk =\\nϵ + (k −1) (k −2) k1\\nk2 −k1Rk+1 + ¯k1Rk+2Rk+1 −(k −1)\\n\\x002k1 −2¯k1Rk+1\\n\\x01\\n+ ¯k1 (k −1) (k −2).\\n(76)\\nWhen we solve the recursion (76) numerically, directly\\nin terms of moments Φk, we begin with a more reﬁned\\nlarge-k approximate form than the ﬁrst-order approxima-\\ntion used as a seed in Eq. (69). The second-order lead-\\ning non-constant approximation, corresponding to the\\nform (70) given for the previous model, is\\nΦk ≈N\\n\\x12k1\\n¯k1\\n\\x13k \\x14\\n1 +\\nη\\nk −1 + η (η/2 −K1)\\nk (k −1)\\n+ O\\n\\x12 1\\nk3\\n\\x13\\x15\\n,\\n(77)\\n20\\n0\\n5\\n10\\n15\\n20\\n25\\n0\\n2\\n4\\n6\\n8\\n10\\nk\\nlog(<hat(n)^{underline(k)}>)\\nupward-going asymptotic recursion from k = 1\\n0\\n50\\n100\\n150\\n200\\n3\\n4\\n5\\n6\\n7\\nk\\nb=<hat(n)^{underline(k+1)}> / <hat(n)^{underline(k)}>; r=hyperbola-fit\\nadjacent ratio against asymptotic analytic model\\nxi\\nepsilon / beta\\n0\\n10\\n20\\n30\\n40\\n3\\n4\\n5\\n6\\n7\\nk\\nb=<hat(n)^{underline(k+1)}> / <hat(n)^{underline(k)}>; r=hyperbola-fit\\nadjacent ratio against asymptotic analytic model\\nxi\\nepsilon / beta\\nFIG. 4:\\nAsymptotic expansions for moments and moment ratios for the model of Fig. 3. First plot shows an asymptotic\\nexpansion for log ˆΦk descaled with ξ =\\np\\nα/β under the recursion relation (68), upward from k = 1. Five traces are generated\\nby starting with ⟨n⟩≡\\n\\nn3\\x0b\\nﬁxed, and\\n\\nn2\\x0b\\nvalues spaced by 1 × 10−10 around the stable value. The group of trajectories\\nbecome uncontrollably divergent by k = 25. Black asterisks are evaluations of the corresponding moments from a Gillespie\\nsimulation, and the value of ⟨n⟩was used to supply the un-speciﬁed normalization c0 ≈2.67 in Eq. (68) for the recursion series.\\nSecond and third panels, which diﬀer only in the plotted range, show ratios ˆΦk+1/ˆΦk, (for which c0 appears only in the lowest\\nterm ⟨n⟩/1) computed by recursion downward from k = 200 with the starting approximation (69) (red curve in the second\\npanel), with Gillespie simulation results overlaid. The Poisson expectation ξ from MFT, and the large-k asymptotic limit ϵ/β,\\nare shown as dashed lines for reference.\\nk2\\nk1\\nε\\nk1\\n_\\nFIG. 5:\\nThis network, for some rate constants, can have\\ntwo non-equilibrium steady states in the mean-ﬁeld approx-\\nimation. Even when this is the case, however, because par-\\nticle number is ﬁnite, the stochastic system always has only\\none ergodically-sampled long-term steady distribution. Some\\nsub-graphs are common with Fig. 3, but the complex graph\\nhas two linkage classes, again giving δ = 1.\\nwhere N is an arbitrary normalization to be ﬁxed by\\nΦ0 = 1, and\\nη ≡k2\\n¯k1\\n−ϵ\\nk1\\n.\\n(78)\\nThe large-k asymptotic behavior of Rk in Eq. (76) can\\nlikewise be solved for an expansion in 1/k about the lead-\\ning ﬁxed point, in the same manner as Eq. (69) for the\\nprevious model.\\nIn this case, the leading departure is\\nO\\n\\x001/k2\\x01\\nrather than O(1/k) as in the 3-complex model.\\nIn general, it can be shown that that the ﬁrst depar-\\nture from whatever ﬁxed point is dictated by the leading\\nlarge-k projection operator in Eq. (34) is determined by\\nthe highest power of k appearing in the expansion in the\\nsum. This order corresponds to the largest stoichiomet-\\nric coeﬃcients for that component of k appearing in the\\nCRN.\\n1.\\nBistability in MFT and handling mixtures of Poisson\\nbasis elements around k = 0\\nThe handling of multi-stability in CRNs with δ > 0 in-\\ntroduces several new interesting properties, both within\\nthe moment recursion relations and in their relation to\\nmean-ﬁeld theory.\\nFirst, MFT will generally predict\\nmulti-stability for Poisson solutions, whether or not the\\nmean particle number is large enough that trajectories\\nin the stochastic process actually generate a multi-modal\\ndensity of particle numbers. The meaning of MFT so-\\nlutions in relation to the analytic structure of represen-\\ntations of the generating function is an interesting topic\\nfrom which we brieﬂy draw results below, but mostly\\nrefer to other developments [35–37, 41] (see also [42],\\nCh. 7). Second and more important, the moment rela-\\ntions are exact, and we therefore expect them to possess\\nunique solutions corresponding to the ergodic distribu-\\ntion, even when mean particle number is large enough\\nthat the MFT representation of multistability corre-\\nsponds to a true incipient16 breaking of ergodicity. The\\nexpansion in high-order moments becomes an important\\nif cryptic representation of the trajectories responsible\\nfor ﬁrst-passages between domains.\\nWe illustrate some of these properties for the case of\\nbistability with a numerical example at parameters: ϵ =\\n16 We say “incipient” because in conventional usage, breaking of\\nergodicity is an asymptotic property, here in the scaling vari-\\nable ⟨n⟩as ⟨n⟩→∞. Formally, ergodicity breaking can still be\\nconsidered well-deﬁned even in ﬁnite-size systems, to the extent\\nthat it is associated with stationary paths in a semiclassical ap-\\nproximation that are essential singularities with respect to the\\nasymptotic expansion in ﬂuctuations [41].\\n21\\n36, k2 = 49; k1 = 14; ¯k1 = 1. The two stable solutions\\nto Eq. (75) are ξ(1) = 1 and ξ(3) = 9, and an unstable\\nsolution exists at ξ(2) = 4.\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\nk\\nlog(< n^{underline(k)} > / xi^k) at xi=9 for bistable model\\ncenter solution for the bistable model\\nFIG. 6:\\nThe solution log ˆΦk to the recursion relations (33),\\ndescaled with ξ(3) = 9, extended downward from k = 200\\nusing the asymptotic approximation (77). The Poisson ba-\\nsis elements for ξ(1) = 1 and ξ(3) = 9 corresponding to the\\nstable solutions in MFT are shown, respectively, in red and\\ngreen, for reference. The values ˆΦk for k ∈0, . . . , 3 match an\\nexpansion (54) with the coeﬃcients (79) and ⟨ΨY (n)⟩′ ≡0.\\nSymbols are from a direct Gillespie simulation.\\nFig. 6 shows the recursive solution for\\n\\nˆnk\\x0b\\n, descaled\\nwith ξ(3) = 9, starting from the large-k asymptotic\\nform (77).\\nThe solution exactly matches the expan-\\nsion (54), with the coeﬃcients given by\\nc1 ≈0.987\\nc2 ≈−0.092\\nc3 ≈0.105.\\n(79)\\nMean-ﬁeld theory suggests no natural interpretation\\nof the mixture (79) with a negative coeﬃcient on an un-\\nstable solution. What would normally be done instead in\\nMFT is to express the mean ⟨n⟩≈1.56 directly as a mix-\\nture of the two MFT-stable values ξ(1) = 1 and ξ(3) = 9\\nwith a mixing coeﬃcient\\nceﬀ≡⟨n⟩−1\\n9 −1\\n≈0.071.\\n(80)\\nWe will use the phenomenological description (80) to un-\\nderstand qualitatively how MFT and stationary-point ex-\\npansions relate to the exact solution of the moment hier-\\narchy.\\n2.\\nInterpretation with a Kramers approximation for\\nﬁrst-passage times\\nThe interpretation of the ergodic solution in terms of a\\nsum over naïve mean-ﬁeld backgrounds can be compared\\nto a stationary-point expansion using the method of in-\\nstantons, which is developed in [35, 37, 42]. Stationary-\\npoint locations and probabilities are governed by the min-\\nima of a non-equilibrium eﬀective potential, which we\\nhave computed for this particular network in [43] (Ch.7),\\nand which takes the form17\\nΞ(¯n) =\\nZ ¯n\\n4\\ndn log\\n\\x12k2n + ¯k1n3\\nϵ + k1n2\\n\\x13\\n.\\n(81)\\nThe extrema of the eﬀective potential are exactly the val-\\nues of the Poisson parameters ξ(γ). (Here we arbitrarily\\nset the zero of the eﬀective potential to n = 4, the saddle\\npoint.) A plot of the eﬀective potential versus n is shown\\nin Fig. 7.\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n-0.2\\n-0.1\\n0\\n0.1\\nbar(n)\\nPhi ( bar(n) )\\nEffective potential for the 1-species, 4-state CRN\\nFIG. 7:\\nThe eﬀective potential Ξ(¯n) from Eq. (81).\\nThe probability to occupy either minimum may be ap-\\nproximated by the Kramers formula [44] derived from Ξ,\\np¯n ∝\\np\\nΞ′′(¯n)e−Ξ(¯n),\\n(82)\\nwhich follows from a semiclassical approximation to the\\nescape rates by the non-trivial stationary trajectories\\nknown as instantons. Fig. 7 shows that for these param-\\neters, the minima of Ξ are ≈−0.25 and ≈−0.19, respec-\\ntively at ¯n = 1 and ¯n = 9, so the Kramers approximation\\nis not expected to be quantitatively accurate. The cor-\\nresponding second derivatives Ξ′′ take values ≈0.07 and\\n≈0.02.\\nFig. 8 shows a timeseries for the particle number n in\\na Gillespie simulation, with dashed lines indicating the\\nminima of the eﬀective potential (81).\\nSome features\\nof the stationary-point approximation are reﬂected: a\\nmajority of the timeseries remains near n = ξ(1) = 1\\nwith strong mean regression, while excursions with mod-\\nest persistence and wider ﬂuctuations occur out to n ∼\\n17 In Ref. [43], we use the notation Φ(¯n) for the eﬀective poten-\\ntial, which we change here to Ξ(¯n) to avoid a collision with the\\nnotation for the moment hierarchy.\\n22\\n 0\\n 5\\n 10\\n 15\\n 20\\n 25\\n 0\\n 200000\\n 400000\\n 600000\\n 800000\\n 1e+06\\n 1.2e+06\\n 1.4e+06\\naverage number of A\\nt\\n 1e-12\\n 1e-10\\n 1e-08\\n 1e-06\\n 0.0001\\n 0.01\\n 1\\n 1\\n 10\\n 100\\nP(na)\\nna\\n\\'data\\'\\nFIG. 8:\\nUpper panel: Timeseries for the particle number n\\nin the 4-state model of Fig. 5. Dashed lines label the minima\\nof the eﬀective potential (81) in the stationary-point expan-\\nsion. Lower panel: histogram of the stationary distribution\\nfor n from the simulations, showing monotone decrease and a\\nshoulder with a mode around n ≈15.\\nξ(3) = 9. However, the excursions do not have the charac-\\nter of fully metastable equilibria. The log of the empirical\\nstationary distribution from the simulations is monotone\\ndecreasing, with a visible shoulder (the signature that\\nexcursions are persistent) with a mode around n ≈15.\\nEq. (82) gives, for the occupation probabilities of the\\ntwo states, approximate values\\np¯9 ≈0.20\\np¯1 = (1 −p¯9) ≈0.80,\\n(83)\\nin which p¯9 corresponds roughly to the empirical mixing\\ncoeﬃcient ceﬀin Eq. (80). The Kramers formula captures\\nthe larger weight on ⟨ΨY (n)⟩(1), but over-estimates the\\nadmixture of ⟨ΨY (n)⟩(3) by about a factor of three.\\nThis CRN was also studied by Anderson et al. [40]. Us-\\ning the fact that the steady state probability ρss is known,\\nthey showed that the non-equilibrium potential, deﬁned\\nhere as −log(ρss), converges to the Lyapunov function\\nfor the corresponding deterministic dynamics, in an ap-\\npropriate scaling limit. The resulting Eq. (38) in [40] is\\nour Eq. (81) obtained as a large-deviation function.\\nD.\\nTwo species, cross-catalysis, and loss of\\nfactorability\\nThe ﬁnal model we will develop shows the loss, for\\nδ > 0, of the factorability which characterizes the steady\\nstates of deﬁciency-0 CRNs under the ACK theorem. We\\nretain the properties already developed, of deviation from\\nPoisson statistics, and the capacity for multistability, by\\nsimply changing the autocatalytic feedback in the model\\nof Fig. 5 to a cross-catalytic feedback between two sym-\\nmetric chemical species.\\nk2\\nε\\nk1\\nk1\\n_\\nk2\\nε\\nk1\\nk1\\n_\\na\\nb\\nFIG. 9:\\nThis CRN uses two species in a cross-catalytic con-\\nﬁguration to produce the same potential for bistability as the\\nnetwork of Fig. 5 produces through 1-species autocatalysis.\\nIn this CRN δ = 2, and the scaling behavior of each species\\nseparately is similar in many respects to 1-species scaling of\\nthe network from Fig. 5.\\nThe resulting multistable network, for two species A\\nand B, is shown in Fig. 9, and its reaction scheme is\\ngiven by\\n∅\\nϵ\\n−⇀\\n↽−\\nk2\\nA\\nA + B\\nk1\\n−⇀\\n↽−\\n¯k1\\n2A + B\\n∅\\nϵ\\n−⇀\\n↽−\\nk2\\nB\\nB + A\\nk1\\n−⇀\\n↽−\\n¯k1\\n2B + A.\\n(84)\\nThe (now vector-valued) rate equation takes the form\\n∂na\\n∂τ = ϵ −k2na + nb\\n\\x00k1na −¯k1n2\\na\\n\\x01\\n∂nb\\n∂τ = ϵ −k2nb + na\\n\\x00k1nb −¯k1n2\\nb\\n\\x01\\n.\\n(85)\\nThe cross-catalytic CRN from Fig. 9 has Liouville op-\\nerator\\nL =\\n\\x001 −a†\\x01 \\x02\\n(ϵ −k2a) +\\n\\x00b†b\\n\\x01 \\x00a†a\\n\\x01 \\x00k1 −¯k1a\\n\\x01\\x03\\n+\\n\\x001 −b†\\x01 \\x02\\n(ϵ −k2b) +\\n\\x00a†a\\n\\x01 \\x00b†b\\n\\x01 \\x00k1 −¯k1b\\n\\x01\\x03\\n. (86)\\nWe introduce pairs of raising and lowering operators\\n\\x00a†, a\\n\\x01\\n,\\n\\x00b†, b\\n\\x01\\n, for the species A and B respectively. The\\nconvention we adopt for ordering the components of the\\n(somewhat complicated) vectors ψ(a, b) and ψ†\\x00a†, b†\\x01\\nis\\ngiven in Eq. (A6), and the corresponding stoichiomet-\\nric decomposition of the Liouville operator is given in\\nEq. (A7).\\n1.\\nMean-ﬁeld solutions and scaling regimes\\nThe truncated factorials Y\\nja\\na\\nand Y\\njb\\nb\\nthat govern the\\nscaling regimes in the steady-state moment hierarchy are\\n23\\nprovided in Eq. (A8).\\nThe lowest-order terms in the\\ndescaled form (51) of the moment equation are\\n\\x12ka\\nξa\\nYae(ka−1) ∂/∂Ya + kb\\nξb\\nYbe(kb−1) ∂/∂Yb\\n\\x13\\nˆA\\nD\\nˆΨY (n)\\nE\\n(87)\\nThe two vanishing conditions for Ya ˆA and Yb ˆA (coming\\nfrom the projectors for the two s-ﬂows of this network)\\nare\\n(ϵ −k2ξa) + ξaξb\\n\\x00k1 −¯k1ξa\\n\\x01\\n= 0\\n(ϵ −k2ξb) + ξaξb\\n\\x00k1 −¯k1ξb\\n\\x01\\n= 0.\\n(88)\\nThese are solved at ξa = ξb = ξ, with ξ again satisfying\\nthe mean-ﬁeld equation (75) for the single-species model\\nof Sec. VI C.\\nIn the other asymptote, the highest-order terms in\\nEq. (51) that are not identically zero in the component-\\nwise product of Y\\nja\\na\\n· Y\\njb\\nb\\nare\\nkakb\\n2ξaξb\\n\\x12ka −1\\nξa\\nY 2\\na · Ybe−∂/∂Ya + kb −1\\nξb\\nYa · Y 2\\nb e−∂/∂Yb\\n\\x13\\nˆAe(ka−1) ∂/∂Ya+(kb−1) ∂/∂Yb\\nD\\nˆΨY (n)\\nE\\n(89)\\nThe vanishing conditions for these two projectors are\\nξaξb\\n\\x00k1 −¯k1ξa\\n\\x01\\n= 0\\nξaξb\\n\\x00k1 −¯k1ξb\\n\\x01\\n= 0\\n(90)\\nThey are again solved at ξa = ξb = ξ but now with\\nξ = k1/¯k1 ≡K1, reproducing the large-k asymptotic\\ncondition from the single-species model of Sec. VI C.\\nThis two-species case may again be solved for mixed\\nmoments in the neighborhood of the diagonal ka = kb,\\nwriting coupled recursion relations for the ratios of the\\nfactorial moments, as shown in [33].\\nIn the following\\nsection, we illustrate an alternate solution method using\\nthe asymptotic expansions that we have developed in the\\nearlier sections.\\n2.\\nPolynomial expansion of a solution for the moment\\nequation in a neighborhood of the diagonal ka = kb\\nWe now illustrate how the representation Λ of the\\ngenerator for the stochastic process, acting similarly to\\na Laplacian on the two-dimensional lattice of moments\\nΦ(ka,kb), can be approximately solved in a neighborhood\\nof the diagonal ka = kb. The method of solution is to use\\nthe symmetry of the recursion equations under ka ↔kb\\nto expand solutions in even powers of (ka −kb), with co-\\neﬃcient functions of (ka + kb) solved by asymptotic ex-\\npansion in a manner similar to that used in the 1-species\\nmodels of Sec. VI B and Sec. VI C.\\nWe do not have a proof that the radius of convergence\\nof these solutions covers the entire lattice of k values,\\nbut comparisons to Gillespie simulation show good agree-\\nment in neighborhoods of the diagonal, suggesting that\\nthe asymptotic boundary conditions we use are consis-\\ntent with those of full solutions. Existence of approxi-\\nmate solutions with this form shows a strong breaking\\nof factorability from the product-Poisson form that is a\\ncharacteristic of the ACK solution for deﬁciency-zero net-\\nworks.\\nIn the following solutions, two combinations of the rate\\nconstants that will appear repeatedly are given a short-\\nhand:18\\nω ≡ϵ/k1\\nη ≡k2/¯k1 −ω.\\n(91)\\nAs in the asymptotic solutions for the 1-species mod-\\nels, we begin by recognizing that the large-k asymp-\\ntotic form is dominated by the scaling of the projection\\noperator that is non-zero for the largest value of j in\\nthe sum (51). This is the projector given in Eq. (89).\\nTherefore we deﬁne the descaled moment operator by\\nΦ(ka,kb) ≡Kka+kb\\n1\\nˆΦ(ka,kb), and look for solutions in the\\nform\\nˆΦ(ka,kb) = N\\n\\x02\\n1 + ηϕ(ka,kb)\\n\\x03\\n,\\n(92)\\nwhere ϕ(ka,kb) →0 at large ka or kb.19\\nWe introduce diagonal and transverse variables, writ-\\nten as functions of the vector argument k:\\nκ(k) ≡ka + kb\\nq(k)2 = (ka −kb)2.\\n(93)\\nIn matrix multiplications below, we will often use κ and\\nq2 as function names, with the argument k which is the\\n18 The second term, η, has already appeared in Eq. (78).\\n19 We justify this assumed scaling by reference to the large-k\\nlimit (77) from the similar 1-species model, because the orders\\nof catalysis are more similar to that case than to the model of\\nSec. VI B leading to the soft (logarithmic) divergence of Eq. (70).\\n24\\nindex of summation suppressed as in usual matrix nota-\\ntion. We look for solutions to ϕ in the form of power\\nseries,\\nϕ =\\n∞\\nX\\nα=0\\nϕ(α)\\nκ q2α.\\n(94)\\nEach term is to be chosen so that ϕ(α)\\nκ\\n→0 as κ →∞.\\nThe functions ϕ(α)\\nκ\\nobey recursion relations similar to\\nthose for an inﬁnite sequence of 1-dimensional moment\\nhierarchies labeled by α, except that the vectors in the\\nsequence are coupled across values of α. Within the so-\\nlution for each ϕ(α)\\nκ , we may treat κ itself as the discrete\\nindex of the recursion.\\nHere as in the 1-species mod-\\nels, the descaled recursion relation suggests leading-order\\nasymptotics for ϕ(α)\\nκ\\nin powers of 1/κ, which may be used\\nto seed numerical solutions.\\nThe conversion from the original lattice ˆΦk to the se-\\nquence of vectors ϕ(α)\\nκ\\nleads to the following approxi-\\nmation procedure to solve for steady states: The steady\\nstate condition from Eq. (51) is\\n0 = ˆΛ [1 + ηϕ] = ˆΛ1 + ηˆΛ\\n∞\\nX\\nα=0\\nϕ(α)\\nκ q2α,\\n(95)\\nwhere 1 ≡[1] [1]T is the dyadic matrix of all 1s.\\nWe\\nintroduce a zeroth order source term s(0)\\nκ\\ndeﬁned by\\nˆΛ1 = −η κ\\nK1\\n≡−ηs(0)\\nκ\\n(96)\\nso the steady-state condition is equivalent to the series\\nsolution of an inhomogeneous Laplacian equation\\nˆΛ\\n∞\\nX\\nα=0\\nϕ(α)\\nκ q2α = s(0)\\nκ ,\\n(97)\\nin which ˆΛ serves as Laplacian and s(0)\\nk\\nis the source for\\nthe inhomogeneous solution.\\nThe form of ˆΛ can be described graphically in terms of\\ndiﬀerence operators acting across adjacent positions on\\nthe lattice of k values, as shown in Fig. 10. ˆΛ acts non-\\ntrivially on q2α as well as on ϕ(α)\\nκ , so Eq. (97) induces\\nconnections across orders in α, and we relegate the details\\nof a solution by successive approximations to App. B.\\n3.\\nProperties of steady states in the 2-species model\\nThe major features of the steady-state solution in this\\nmodel, which we have veriﬁed against Gillespie simula-\\ntions, are the following:\\nOrder of terms versus α: The naïve scaling dimen-\\nsions implied for ϕ(α)\\nκ\\nby Eq. (51) suggest that these func-\\ntions should decay at large κ with increasing powers of\\n1/κ. Numerically, this appears to be borne out, with in-\\ndeed the entire series ϕ(α)\\nκ\\ndecreasing in magnitude with\\nincreased α. In addition to the on-diagonal terms (where\\nq2 ≡0), which are deﬁned entirely in terms of ϕ(0)\\nκ , terms\\nadjacent to the diagonal, which should be dominated by\\nϕ(α)\\nκ\\nat low orders in α, are well approximated by the\\nsolution ϕ(0)\\nκ\\nacross the whole range of ka = kb.\\nScaling of ﬁnite-order approximations along rays\\nof |q| /κ:\\nThe measure of error – non-zero values of\\n∂Φk/∂τ – appears roughly constant along rays of ﬁxed\\n|ka −kb| / (ka −kb) ≡|q| /κ at ﬁnite orders of approxi-\\nmation in ϕ(α)\\nκ . Stabilizing the asymptotic expansion in-\\ndependently at each order of ϕ(α)\\nκ\\nbecomes increasingly\\ndiﬃcult as α increases, due to cross-level feedback and\\nthe successive-approximation algorithm we use for so-\\nlution.\\nThus we obtain an approximate solution only\\nthrough order ϕ(5)\\nκ .\\nComparison of the 2-species cross-catalytic and 1-\\nspecies autocatalytic models: The ratios of adjacent\\nmoments in the value ka+kb – which require at minimum\\ncomparing on-diagonal and ﬁrst-oﬀ-diagonal moments –\\nare shown in Fig. 11 and compared to the corresponding\\nsequence of ratios derived from the moment solutions in\\nFig. 6. We ﬁnd that the mean value ⟨na⟩≡⟨nb⟩in the 2-\\nspecies model is very close to the mean value ⟨n⟩from the\\n1-species model, as suggested by the equivalence of their\\nmean-ﬁeld forms, even though both models diﬀer signif-\\nicantly from the MFT-approximation, which is the solu-\\ntion ξ to Eq. (75). Moreover, the second moments ⟨nanb⟩\\nremain close to the 1-species expectation ⟨n (n −1)⟩, and\\nagain diﬀerent in both cases from the MFT prediction.\\nAt higher k, a diﬀerent behavior is seen: the transition\\nto scaling dominated by the term (89) is governed in the\\n2-species model by ka and kb comparable to 1-species k,\\nand not by the sum ka +kb. This is expected by compar-\\ning the forms of the two Liouville operators (73) and (86).\\n4.\\nBreaking of the factorability of the ACK theorem through\\ncross-catalysis\\nThe solution scheme deﬁned in Sec. VI D 2 and worked\\nout in App. B 1 suggests that the moment hierarchy near\\nthe diagonal is well approximated (at least at large k) by\\na function of ka + kb, which is a strong deviation from\\nthe factorability that would be produced by the ACK\\ntheorem for a deﬁciency-0 network. To study the failure\\nof factorability more directly than through the numerical\\napproximation scheme of App. B 1, we may alternatively\\napproximate the large-k behavior of the moment hierar-\\nchy by a sum of products of powers of 1/ka and 1/kb,\\nsolved fully for ˆΛˆΦ = 0 order-by-order in 1/k. The two\\nexpansions do not have the same asymptotics along the\\nboundaries ka = 0 and kb = 0, but they can be made\\nto satisfy the same boundedness criteria at large k in a\\nneighborhood of ka = kb.\\n25\\nωka\\nka(ka-1)K1\\nωkb\\nkb(kb-1)K1\\nkakb(kb-1)\\n \\nkakb(ka-1)\\n \\nkakbK1\\nkakbK1\\nka\\nkb\\nka - 1\\nkb - 1\\nka + 1\\nkb + 1\\n−η(ka + kb)\\nkaK12\\nkbK1\\n2\\nFIG. 10:\\nGraphic representation of the action of the generator ˆΛ given by Eq (51) for the CRN of Fig. 9. The grid represents\\nadjacent values of the index pair (ka, kb), (ka ± 1, kb), (ka, kb ± 1), and (ka ± 1, kb ± 1), as indicated by axis labels in the ﬁrst\\npanel. The solid dot in the ﬁrst panel represents multiplication by the moment at (ka, kb). Solid arrows indicate multiplication\\nof moments by the projection vector [ 1 -1 ], acting on the moments at the tip and tail of the arrow. The labels on the lines\\nindicate the function of parameters and k values that multiplies each such projector. Thus the ﬁrst panel is simply the term\\n−η (ka + kb) ˆΦ(ka,kb), etc. The ﬁrst and second panels, and the lower-right pair of terms in the third panel, come from the\\nterms at order ja + jb = 1 in Eq. (51), given in Eq. (87). The terms running anti-diagonally through the center in the third\\npanel come from the terms at order ja + jb = 2, and the terms in the upper-left corner of the third panel come from the terms\\nat order ja + jb = 3, shown in Eq. (89). The large-k asymptotic Poisson ˆΦ →N1 is annihilated identically by both panels with\\narrows.\\n0\\n5\\n10\\n15\\n20\\n25\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nk_a + k_b == k\\nhat(Psi)_k/hat(Psi)_{k-1}\\ndiagonal and first off-diagonal hatted-moment ratios at q^0 order\\n0\\n5\\n10\\n15\\n20\\n25\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nb = k_a + k_b == k (twospec); r = k (onespec)\\nhat(Psi)_k/hat(Psi)_{k-1}\\ncompare 1- and 2-species moment ratios at comparable order\\n0\\n5\\n10\\n15\\n20\\n25\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nb = (k_a + k_b)/2 == k/2 (twospec); r = k (onespec)\\nhat(Psi)_k/hat(Psi)_{k-1}\\ncompare 1- and 2-species moment ratios at comparable order\\nFIG. 11:\\nComparison of Gillespie simulations (asterisks) for the CRN model of Fig. 9, to the leading solution ϕ(0)\\nκ\\n(blue\\ncurve) from Eq. (94).\\nFirst panel shows ratios ˆΦ(ka,kb)/ˆΦ(ka−1,kb) = ˆΦ(ka,kb)/ˆΦ(ka,kb−1) for ka = kb (black crosses), and\\nˆΦ(ka−1,kb)/ˆΦ(ka−1,kb−1) = ˆΦ(ka,kb−1)/ˆΦ(ka−1,kb−1) for ka = kb (red crosses).\\nSecond and third panels show comparisons\\nbetween these adjacent-moment ratios in the 2-species model and moment ratios ˆΦk/ˆΦk−1 in the 1-species model of Fig. 5\\n(red curve) that has an equivalent MFT solution. Second panel plots ka + kb directly against one-species k, showing that for\\nthe lowest moments total particle number controls similar moment values in the two models. Third panel plots (ka + kb) /2\\n(equivalently, ka or kb)ăagainst 1-species k, showing that the transition to scaling dominated by the autocatalytic reactions is\\ngoverned independently by ka and kb.\\nAn expansion of the solution to ˆΛˆΦ = 0 to second order\\nin 1/k is given by\\nΦ(ka,kb) ≈NKka+kb\\n1\\n\\x14\\n1 +\\nη\\nka + (K1 −η) /2 +\\nη\\nkb + (K1 −η) /2 +\\nη (η −K1)\\n[ka + (K1 −η) /2] [kb + (K1 −η) /2]\\n\\x15\\n≈NKka+kb\\n1\\n\" \\n1 +\\nη\\nka + K1/2 +\\nη2/2\\n(ka + K1/2)2\\n!  \\n1 +\\nη\\nkb + K1/2 +\\nη2/2\\n(kb + K1/2)2\\n!\\n−\\nηK1\\n(ka + K1/2) (kb + K1/2)\\n\\x15\\n,\\n(98)\\n26\\nwhich we have checked numerically cancels the error term\\n∂ˆΦ/∂τ to the correct order of 1/k.20 The ﬁrst departure\\nfrom factorability occurs in the second-order term with\\nnumerator −ηK1, showing where the δ-ﬂow contributions\\ncreate correlated ﬂuctuations that would be ruled out in\\na (δ = 0)-network.\\n5.\\nGeneralizing to a larger number of species\\nOur constructions apply to CRNs with arbitrary num-\\nbers of species, but the foregoing models show how the\\ncharacter of solution methods changes with increasing\\nnumbers. Because the generator (34) has ﬁnitely-many\\nterms for any ﬁnite CRN, for one-species problems, the\\nnumber of undetermined boundary data that must be\\nsampled to search for stable asymptotic expansions is\\nalways ﬁnite.\\nMoreover, the large-k limit may be ex-\\ntended to improve the precision of approximations from\\ncoarser seed functions. In two or more dimensions, un-\\nknown boundary data can exist along all surfaces of co-\\ndimension 1 or more, in which one or more kp = 0. If an\\nasymptotic bounding surface is moved outward for the\\nnonzero values of kp, new unknown values are added to\\nthe set that must be sampled along small-kp boundaries.\\nTherefore increasingly much of the information in a solu-\\ntion must come from boundary-condition data, compared\\nto the constraint in the scalar condition ΛΦ = 0.\\nOther problems of convex analysis, analogous to the\\nFeinberg deﬁciency-zero argument, are also left as ques-\\ntions for future work. Is there a systematic way to rep-\\nresent the number of distinct scaling regions controlled\\nby terms in the sum (34) over j? Do large-k asymptotic\\nconditions on the vector ξ of coherent-state parameters\\nalways possess unique solutions? When are they under-\\ndetermined, and in these cases do the solutions from s-\\nﬂow conditions extend outward indeﬁnitely?\\nDespite leaving several detailed questions to be ad-\\ndressed, we emphasize that the ﬁnite rank of the op-\\nerator Λ reduces the solving of a moment hierarchy to\\nall orders, to a problem of equal complexity to solving\\na Laplacian diﬀusion equation, generally with beyond-\\nnearest-neighbor couplings. This is a simpler and less-\\ncostly problem than direct simulation, especially for high-\\n20 Eq. (98) may be compared to Eq. (77) for the one-species model,\\nand also to the leading-order scaling estimate for the term ϕ(0)\\nκ\\nfrom Eq. (B6). Along the diagonal ka = kb, Eq. (98) becomes\\nΦ(ka,kb) ≈NKka+kb\\n1\\n\\x14\\n4η (ka + kb)\\n(ka + kb + K1 −η)2 .\\n\\x15\\nThe leading behavior diﬀers from Eq. (B6) by a factor (4/3)\\nmultiplying (K1 −η), which is consistent with the fact that the\\nscaling solutions in App. B 1 only propagate the eﬀects of ˆΛ up-\\nward in a hierarchy of powers; feedbacks down the hierarchy are\\nabsorbed in higher-order correction terms that have the same\\nlarge-k order as corrections in the multiplier of (K1 −η).\\norder moments in systems with large particle numbers.\\nVII.\\nCONCLUSIONS\\nThe new observations and results of this work may be\\ngrouped under the following four main topics:\\nA shift in emphasis from topology to dynamics:\\nIn this article we have bypassed the use of deﬁciency as a\\ntopological index to categorize networks, and focused in-\\nstead on the contrast between mean-regressing and non-\\nmean-regressing ﬂows on the complex graph, which is the\\ndynamical property that causes deﬁciency to be impor-\\ntant.\\nThe dynamical distinction, which we express in\\nthe stoichiometric representation (40) of the stochastic-\\nprocess generator, continues to be deﬁnable in terms of\\nthe images and kernels of A and Y A, even in (δ > 0)-\\nnetworks where it cannot be associated with a deﬁciency-\\n0 sub-network. As shown in Sec. V C, the unique role\\nof the mean-regressing ﬂows as the determinants of the\\nﬁrst-moment conditions, which persists to all orders in\\ndeﬁciency-0 networks, persists in a more limited form as\\nthe leading term in a 1/n-expansion for the determina-\\ntion of moments in more general networks.\\nThe Doi operator algebra, Laplace transforms,\\nand an expanded role for Poisson basis distribu-\\ntions: The Doi operator algebra used to express the gen-\\nerators of the stochastic process is the tool that allows\\nus to associate the s-ﬂows and δ-ﬂows in the stoichiomet-\\nric representation with corresponding product-Poisson\\ndistributions, the origin and meaning of which are the\\nsame as those of the unique steady-state distributions in\\nthe Anderson-Craciun-Kurtz theorem. In this way not\\nonly the linear algebra of ﬁrst moments in the stoichio-\\nmetric subspace, but the Poisson family of distributions\\nas basis functions, extends directly from deﬁciency-0 to\\ndeﬁciency-nonzero cases. This simpliﬁcation and clariﬁ-\\ncation results from working with the Laplace transform\\nand the Liouville operator: the elementary projection op-\\nerators in terms of which L naturally decomposes, which\\nannihilate particular Poisson distributions, describe col-\\nlective motions that recursively relate all orders in the\\nmoment hierarchy.\\nThe manifestation of fundamental symmetries in\\nthe generator of the stochastic process: The Doi\\noperator algebra also exposes symmetries of the genera-\\ntors (27) for stochastic CRNs that are obscured in the\\ncombination of index shifts and number-dependence of\\nrates in the master equation (26), and masked entirely\\nin the asymmetric form of the Mean-Field mass-action\\nrate equations (41,44). We use the simpliﬁcations this\\nformalism aﬀords to both derive the moment hierarchy\\nas well as develop approximations to solve them.\\nLocality of scaling regions and connection to the\\nPoisson approximations: The ﬁniteness of the gen-\\nerator acting on the moment hierarchy is the feature\\n27\\nthat allows scaling regimes to be deﬁned locally in dif-\\nferent asymptotic ranges of moments, and that makes\\nPoisson-form basis distributions good approximations to\\nthe moment recursion relations in such regions. Despite\\nthe fundamental underlying complexity of CRNs – re-\\nmember that the search for non-locally deﬁned topolog-\\nical properties such as shortest paths or feedback cycles\\ncan be NP-hard [13] – the convergence of both high- and\\nlow-k asymptotic expansions toward the matching region\\nbuﬀers the strength with which diﬀerent regions are cou-\\npled, and allows the convergence toward the Poisson basis\\nelements to be locally governed. n this way, Poisson dis-\\ntributions which are among the lowest-information dis-\\ntributions, serve as a basis for the evaluation of the mo-\\nments in systems with the potential for very high in-\\nformation capacity. As a by-product, we also obtain a\\nsystematic approach to moment closure, which does not\\nhave any of the problems of ad-hocness and unphysical\\nresults [45] prevalent in existing schemes.\\nAcknowledgments\\nDES thanks the Physics Department at Stockholm\\nUniversity for support during visits in 2014 and 2016\\nwhen the bulk of this work was carried out.\\nWe also\\nacknowledge Dan Rockmore, Scott Pauls, and Greg Lei-\\nbon for hospitality and conversations on related topics\\nin 2010, Artur Wachtel in 2015 and Nathaniel Virgo of\\nELSI in 2016.\\nAppendix A: Supporting algebra for CRN examples\\nThis appendix provides explicit forms for transfer ma-\\ntrices, Liouville operators, and truncated factorials of the\\nstoichiometric matrix, for the CRN models in the main\\ntext.\\n1.\\nForms of the transfer matrices appearing in\\nmaster equations\\nTransfer matrices are given below for the indicated\\nCRN models:\\nFor the graph of Fig. 2 the evolution equation for the density that deﬁnes the transfer matrix has the form\\n˙ρn =\\nh\\x10\\ne−∂/∂n −1\\n\\x11\\nαn +\\n\\x10\\ne∂/∂n −1\\n\\x11\\nβn (n −1) (n −2)\\ni\\nρn.\\n(A1)\\nThis equation illustrates in the simplest form how Poisson steady states come to be ruled out when the ACK theorem\\nno longer applies. The shift operator acts by single units n →n ± 1, but the numerical factors in the rates for particle\\ncreation and annihilation diﬀer by second-order terms in n, which cannot be absorbed in any Poisson distribution.\\nFor the weakly reversible graph of Fig. 3, the master equation adds a term aﬀecting ﬂuctuations though it preserves\\nthe ﬁrst-moment rate equation:\\n˙ρn =\\nn\\x10\\ne−∂/∂n −1\\n\\x11\\n[αn + ϵn (n −1)] +\\n\\x10\\ne∂/∂n −1\\n\\x11\\n[ϵn (n −1) + βn (n −1) (n −2)]\\no\\nρn.\\n(A2)\\nThe master equation for the CRN of Fig. 5 is\\n˙ρn =\\nn\\x10\\ne−∂/∂n −1\\n\\x11\\n[ϵ + k1n (n −1)] +\\n\\x10\\ne∂/∂n −1\\n\\x11 \\x02\\nk2n + ¯k1n (n −1) (n −2)\\n\\x03o\\nρn\\n(A3)\\nFor the CRN of Fig. 9, n becomes a two-component index to ρ, and the master equation becomes\\n˙ρn =\\nn\\x10\\ne−∂/∂na −1\\n\\x11\\n[ϵ + k1nbna] +\\n\\x10\\ne∂/∂na −1\\n\\x11 \\x02\\nk2na + ¯k1nbna (na −1)\\n\\x03\\n+\\n\\x10\\ne−∂/∂nb −1\\n\\x11\\n[ϵ + k1nanb] +\\n\\x10\\ne∂/∂nb −1\\n\\x11 \\x02\\nk2nb + ¯k1nanb (nb −1)\\n\\x03o\\nρn\\n(A4)\\n2.\\nStoichiometric representation forms for Liouville operators of the models\\nThe stoichiometric decompositions of Liouville operators not explicitly written in the main text are provided here:\\n28\\na.\\nThe 1-species, 4-complex model of Sec. VI C\\nThe diagonalized form of the Liouville operator (73), corresponding to master equation (A3) is\\nL =\\nh\\n1 a† a†2 a†3 i\\n1\\nϵ2 + k2\\n2 + k2\\n1 + ¯k2\\n1\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n\\uf8eb\\n\\uf8ec\\n\\uf8ed\\n\\x00ϵ2 + k2\\n2\\n\\x01\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n−1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fb+\\n\\x00k2\\n1 + ¯k2\\n1\\n\\x01\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n0\\n0\\n1\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f8\\n\\x02\\nϵ −k2 k1 −¯k1\\n\\x03\\n+\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n−1\\n−1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n\\uf8eb\\n\\uf8ed\\n\\x00k2\\n1 + ¯k2\\n1\\n\\x01 \\x02 ϵ −k2 0 0 \\x03\\n−\\n\\x00ϵ2 + k2\\n2\\n\\x01 \\x02\\n0 0 k1 −¯k1\\n\\x03 \\uf8f6\\n\\uf8f8\\n\\uf8fc\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8fe\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\na\\na2\\na3\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n(A5)\\nin which the top line is the s-ﬂow and the bottom line is the δ-ﬂow.\\nb.\\nThe 2-species, cross-catalytic model of Sec. VI D\\nFor the 2-species model of Fig. 9, the vector of creation and annihilation operators with respect to which we will\\nwrite L in matrix form is:\\nψ† =\\nh\\n1 a† b† a†b† a†2b† a†b†2 i\\n(ψ)T =\\n\\x02 1 a b ab a2b ab2 \\x03\\n(A6)\\nThen L from Eq. (86), corresponding to the master equation (A4), in matrix form and also diagonalized, becomes\\nL = ψ†\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n1\\n2\\n\\x002ϵ2 + k2\\n2 + 2k2\\n1 + ¯k2\\n1\\n\\x01\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\x002ϵ2 + k2\\n2\\n\\x01\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n−1\\n−1\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+\\n\\x002k2\\n1 + ¯k2\\n1\\n\\x01\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n0\\n0\\n2\\n−1\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\x02\\n2ϵ −k2 −k2 2k1 −¯k1 −¯k1\\n\\x03\\n+\\n1\\n2\\n\\x00k2\\n2 + ¯k2\\n1\\n\\x01\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nk2\\n2\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n−1\\n1\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n+ ¯k2\\n1\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n0\\n0\\n0\\n−1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\x02\\n0 −k2 k2 0 −¯k1 ¯k1\\n\\x03\\n+\\n1\\n2\\n\\x002ϵ2 + k2\\n2 + 2k2\\n1 + ¯k2\\n1\\n\\x01\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n2\\n−1\\n−1\\n−2\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\x002k2\\n1 + ¯k2\\n1\\n\\x01 \\x02 2ϵ −k2 −k2 0 0 0 \\x03\\n−\\n\\x002ϵ2 + k2\\n2\\n\\x01 \\x02\\n0 0 0 2k1 −¯k1 −¯k1\\n\\x03\\n+\\nk2¯k1\\n2\\n\\x00k2\\n2 + ¯k2\\n1\\n\\x01\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n−1\\n1\\n0\\n1\\n−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n\\x02\\n0 −¯k1 ¯k1 0 k2 −k2\\n\\x03 \\uf8fc\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fd\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8fe\\nψ\\n(A7)\\nThe top line is the s-ﬂow projected out by\\n\\x00a†a + b†b\\n\\x01\\n; the second line is the s-ﬂow projected out by\\n\\x00a†a −b†b\\n\\x01\\n. The\\nnext two lines are the δ-ﬂows with the same symmetry or antisymmetry.\\n29\\nThe stoichiometric vectors appearing in Eq. (51), and their products with A are\\nYa ≡Y 1\\na =\\n\\x02 0 1 0 1 2 1 \\x03\\nY 1\\na A =\\n\\x02\\nϵ −k2 0 k1 −¯k1 0\\n\\x03\\nYb ≡Y 1\\nb =\\n\\x02 0 0 1 1 1 2 \\x03\\nY 1\\nb A =\\n\\x02\\nϵ 0 −k2 k1 0 −¯k1\\n\\x03\\nYa · Yb =\\n\\x02 0 0 0 1 2 2 \\x03\\n(Ya · Yb) A =\\n\\x02\\n0 0 0 2k1 −¯k1 −¯k1\\n\\x03\\nY 2\\na =\\n\\x02 0 0 0 0 2 0 \\x03\\n= Y 2\\na · Yb\\nY 2\\na A = 2\\n\\x02\\n0 0 0 k1 −¯k1 0\\n\\x03\\n=\\n\\x00Y 2\\na · Yb\\n\\x01\\nA\\nY 2\\nb =\\n\\x02 0 0 0 0 0 2 \\x03\\n= Y 2\\nb · Ya\\nY 2\\nb A = 2\\n\\x02\\n0 0 0 k1 0 −¯k1\\n\\x03\\n=\\n\\x10\\nY 2\\nb · Ya\\n\\x11\\nA\\n(A8)\\nAppendix B: Polynomial expansion for solutions to\\nthe two-species CRN of Sec. VI D\\nThis appendix describes the combination of recursive\\nsolution, and successive approximation, used to solve the\\nhierarchical expansion (94) for the lattice of moments in\\nthe two-species model of Sec. VI D 2.\\nHere, to simplify notation and improve readability, we\\nwill regard matrices such as ˆΛ as operators that shift\\nthe indices (ka, kb) in terms ϕ(α)\\nk q2α by means of discrete\\nindex-shift operators e∂/∂ka, e∂/∂kb, as we did for transfer\\nmatrices in Eq. (26) et seq.\\nBecause of the exchange\\nsymmetry in the dynamical equations under ka ↔kb, ˆΛ\\nacts on ϕ(α)\\nk\\nthrough a shift of the κ value (by integers),\\nand on q2α through shifts in α. This allows us to treat κ\\nas an index shifted by integers, analogous to k in single-\\nspecies models. Where we suppress subscript κ indices,\\nthe whole vector is intended.\\nWith these notational conventions, the action of the\\ngenerator in Eq. (97) can be broken down into three\\nterms:\\nˆΛ\\n\\x10\\nϕ(α)q2α\\x11\\n=\\n\\x10\\nˆΛ0ϕ(α)\\x11\\nq2α−s(α+1)q2(α+1)+\\nα−1\\nX\\nβ=0\\nσ(α)\\nβ q2β\\n(B1)\\nHere ˆΛ0 is a diagonal operator (in α) acting only on ϕ(α),\\nwhich takes the form (refer to Fig. 10):\\n\\x10\\nˆΛ0ϕ(α)\\x11\\nκ =\\n\"\\x00κ2 −θα>0\\n\\x01\\n(κ −2 −2α) −2αθα>0\\n4K1\\n+ ω\\nK1\\n(κ −2α)\\n# \\x10\\nϕ(α)\\nκ−1 −ϕ(α)\\nκ\\n\\x11\\n−\\n\\x14\\nκ\\n\\x12 η\\nK1\\n+ 2α\\n\\x13\\n+ 2α\\n\\x12 ω\\nK1\\n−2α\\n\\x13\\n+ 2ακ2\\n4K1\\n+ θα>0\\nκ −2\\n4K1\\n\\x15\\nϕ(α)\\nκ\\n+ [(κ −2α) (κ −1)]\\n\\x10\\nϕ(α)\\nκ\\n−ϕ(α)\\nκ+1\\n\\x11\\n+ κK1\\n\\x10\\nϕ(α)\\nκ+1 −ϕ(α)\\nκ+2\\n\\x11\\n−2αK1ϕ(α)\\nκ+1\\n(B2)\\nIn the same way as the action of ˆΛ on the constant background 1 produced the zero-th order source −ηs(0) in Eq. (97),\\nthe action on each order ϕ(α) generates a source term s(α+1) in Eq. (B1) that propagates cross-terms one order upward\\nin α, deﬁned by\\ns(α+1)\\nκ\\n=\\n1\\n4K1\\nh\\n(κ −2)\\n\\x10\\nϕ(α)\\nκ−1 −ϕ(α)\\nκ\\n\\x11\\n−2αϕ(α)\\nκ−1\\ni\\n(B3)\\nIn addition to an upward-propagating “source” term, Eq. (B1) contains “feedback” terms, which propagate cross-terms\\ndownward from order q2α to all lesser orders q2β with β < α, given by\\nσ(α)\\nβ\\n=\\n\\x14\\nκ\\n\\x12\\n2α\\n2β\\n\\x13\\n−\\n\\x12\\n2α\\n2β −1\\n\\x13\\x15 \\x12 ω\\nK1\\nϕ(α)\\nκ−1 + [K1 −(κ −1)] ϕ(α)\\nκ+1\\n\\x13\\n+ 22(α−β)\\n\\x14κ (κ −2)\\n2\\n\\x12\\n2α\\n2β\\n\\x13\\n−2 (κ −1)\\n\\x12\\n2α\\n2β −1\\n\\x13\\n+ 2\\n\\x12\\n2α\\n2β −2\\n\\x13\\x15\\nϕ(α)\\nκ\\n+\\n1\\n4K1\\n\\x1a\\nκ2\\n\\x14\\n(κ −2)\\n\\x12\\n2α\\n2β\\n\\x13\\n−\\n\\x12\\n2α\\n2β −1\\n\\x13\\x15\\n−\\n\\x14\\n(κ −2)\\n\\x12\\n2α\\n2β −2\\n\\x13\\n−\\n\\x12\\n2α\\n2β −3\\n\\x13\\x15\\x1b\\nϕ(α)\\nκ−1\\n(B4)\\nIn Eq. (B4), referring to the graphical form of Fig. 10,\\nall terms from ja + jb = 1 are grouped in the ﬁrst line,\\n30\\nall terms from ja +jb = 2 are grouped in the second line,\\nand all terms from ja + jb = 3 are grouped in the third\\nline.\\n1.\\nSolution by upward propagation in powers of q2\\nand perturbative correction in 1/κ\\nThe decomposition (B1) of the steady-state condition\\ncan now be solved by alternating steps of exact cancella-\\ntion of source terms at ascending orders in α, and succes-\\nsive approximation to cancel feedback terms which takes\\nthe form of a perturbative expansion in 1/κ.\\nUpward-propagation consists of solving a series of\\nLaplacian equations in the operator ˆΛ0 in terms of the\\nsources (B3), as\\nˆΛ0ϕ(α) = s(α)\\n(B5)\\nfor all α ≥0.\\nExamination of the scaling terms in κ in Eq. (B2) sug-\\ngests that a bounded large-κ asymptotic approximation\\nfor each order is\\nϕ(α)\\nκ\\n=\\n4\\n(κ + 4 (K1 −η) /3)2α+1 + O\\n\\x12\\n1\\nκ2α+2\\n\\x13\\n.\\n(B6)\\nSolution of Eq. (B5) at each order α by one-dimensional\\nmatched asymptotic expansion proceeds as for the 1-\\nspecies models. At order α = 0, the solution can be sta-\\nbly extended down to κ = 0, but for α > 1 the asymptotic\\napproximation (B6) is not suﬃcient to produce conver-\\ngence below κ ∼20, and a non-trivial matched expansion\\nis needed to produce valid higher-order corrections in q2\\nto the low-order moments.\\nOne round of forward propagation (through all or-\\nders α) will produce a solution that does not satisfy\\nˆΛ P∞\\nα=0 ϕ(α)q2α −s(0) = 0, but rather\\nˆΛ\\n∞\\nX\\nα=0\\nϕ(α)q2α −s(0) =\\n∞\\nX\\nα=0\\nα−1\\nX\\nβ=0\\nσ(α)\\nβ q2β\\n(B7)\\nThe full solution can be approached perturbatively by us-\\ning the feedback terms on the right-hand side of Eq. (B7)\\nas sources for an iterative correction to the original ϕ.\\nA successive-approximation approach to full solutions ϕ(α)\\nA scaling analysis following from Eq. (B6) suggests\\nthat the correction terms needed to cancel the residu-\\nals in Eq. (B7) are suppressed by powers of 1/κ, and\\nthus that a method of successive approximations should\\nconverge.\\nNo feedback term σ(α)\\n0\\nto q0 order from α = 0 exists,\\nbecause β ≤α −1 in the sum (B1). For all α ≥1, σ(α)\\n0\\nis given by\\n\\x10\\nσ(α)\\n0\\n\\x11\\nκ =\\n\\x12 ωκ\\nK1\\n+ κ2 (κ −2)\\n4K1\\n\\x13\\nϕ(α)\\nκ−1\\n+ κ [K1 −(κ −1)] ϕ(α)\\nκ+1 + 22α κ (κ −2)\\n2\\nϕ(α)\\nκ .\\n(B8)\\nSince the lowest-order contribution is from ϕ(1)\\nκ\\n∼4/κ3,\\nit follows that σ(α)\\n0\\nis no larger for any α than the leading\\nterm\\n\\x10\\nσ(1)\\n0\\n\\x11\\nκ = 1\\nK1\\n+ O\\n\\x12K1\\nκ2\\n\\x13\\n+ O\\n\\x12 1\\nκ\\n\\x13\\n.\\n(B9)\\none order lower (in either K1 or κ, according to the range\\nof κ) than the scaling of s(0) in Eq. (96).\\nThe same\\nargument extends to higher β in Eq. (B7); the ﬁrst term\\nthat contributes at each order scales with two additional\\npowers of 1/κ, and so is smaller than the corresponding\\ns(α) term in the ﬁrst iteration of Eq. (B3).\\nWe therefore introduce a second-order correction term\\nϕ(α)′, satisfying\\nˆΛ\\n∞\\nX\\nα=0\\nϕ(α)′q2α = −\\n∞\\nX\\nα=0\\nα−1\\nX\\nβ=0\\nσ(α)\\nβ q2β +\\n∞\\nX\\nα=0\\nα−1\\nX\\nβ=0\\nσ(α)′\\nβ\\nq2β,\\n(B10)\\nwhere ϕ(α)′ is solved by upward propagation in q2α, as\\nin Eq. (B5), but now with an entire tower of sources\\n−P∞\\nα=β+1 σ(α)\\nβ\\nat each order q2β, rather than just the\\nzeroth-order source s(0) that was used for ϕ(α), and leav-\\ning its own residues σ(α)′. After inﬁnitely many itera-\\ntions, the residue terms go to zero as a sequence in pow-\\ners of 1/κ, and if the sequence converges, the sum over\\ncorrections will be a solution to the original steady-state\\ncondition (97).\\nAn equivalent expression for the closed solution (sum-\\nming over all orders of perturbative correction), ex-\\npressed in terms of the homogeneous operator ˆΛ0, is\\n∞\\nX\\nα=0\\n\\x10\\nˆΛ0ϕ(α) −s(α)\\x11\\nq2α = −\\n∞\\nX\\nα=0\\nα−1\\nX\\nβ=0\\nσ(α)\\nβ q2β.\\n(B11)\\nEq. (B11) is similar in form to a Schwinger-Dyson\\nequation\\nfor\\nGreen’s\\nfunction\\nsolution,\\nin\\nwhich\\n\\x10\\nˆΛ0ϕ(α) −s(α)\\x11\\nserves as a “bare” Green’s function,\\nwhich deﬁnes a basis for perturbative incorporation of\\nan “interaction” term P∞\\nα=β+1 σ(α)\\nβ .\\n2.\\nNumerical evaluations\\nWe have implemented the above solution method for a\\nseries (94) truncated at ﬁve successive orders of approx-\\nimation ϕ(αmax) for αmax = 0, . . . , 4. Under perfect con-\\nvergence of the perturbative recurrence (B10), the error\\n31\\nmeasure ∂ˆΦ(ka,kb)/∂τ would cancel to order q2(αmax+1)\\naround the diagonal (order q10 for the highest-order ap-\\nproximation we compute). We obtain cancellation along\\nthe diagonal to machine precision for the lowest-order\\ncorrection ϕ(0), but instability of the downward-going\\nasymptotic expansion at higher orders degrades both the\\naccuracy of ˆΦ(ka,kb) on the diagonal for ka +kb ≲20, and\\nconvergence toward the q2(αmax+1) residual.\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nk_a + k_b\\nhat(Psi)_{(k,k)}/hat(Psi)_{(k-1,k-1)}, bgrcm = 01234\\ndiagonal hatted-moment ratios at q^2 orders 01234, k* = recurs; ro = sims\\nFIG. 12:\\nRatios of moments ˆΦ(ka,kb)/ˆΦ(ka−1,kb−1) plotted\\nversus ka+kb ≡k, along the diagonal ka = kb. The series (94)\\nis truncated at ﬁve successive orders of approximation ϕ(αmax)\\nfor αmax = 0, . . . , 4 (color sequence bgrcm). Symbols are the\\nratios obtained directly from sampled moments of a station-\\nary Gillespie simulation, showing that the order αmax = 0\\nprovides a good approximation to the diagonal moments, as\\nshown in Fig. 11 of the main text. Divergence of higher-order\\nterms – which occurs with opposite sign for odd versus even\\nαmax – reﬂects instability of the asymptotic expansion down-\\nward from large κ.\\nSo far I have not implemented a\\nmatched asymptotic expansion for these terms.\\nFig. 12 shows the successive approximations to the\\nratio ˆΦ(ka,kb)/ˆΦ(ka−1,kb−1) along the diagonal ka = kb,\\ncompared to values obtained from a Gillespie simulation.\\nThe approximation αmax = 0 already shows good agree-\\nment with simulations.\\nDivergence of the downward-\\ngoing asymptotic expansion for higher-order terms begins\\naround ka = kb ≈9, which is the upper stable solution\\nfor the coherent-state mean number (75), as predicted in\\nthe scaling analysis of Sec. V B.\\nFig. 13 shows the error measure ∂ˆΦ(ka,kb)/∂τ (the de-\\nviation from a full steady-state condition) across the anti-\\ndiagonal contour ka + kb = 49, testing the quality of the\\nconvergence of the ϕ(αmax) approximation. This contour\\nis in a range ka, kb ≫9 where the asymptotic expansions\\nare still fairly well-controlled. The qualitative character\\nof the convergence is well approximated along the diag-\\nonal, but exact cancellation to order q2(αmax+1) degrades\\nat higher αmax, as both the asymptotic expansion and\\nthe recursive solution (B10) accumulate numerical errors.\\nThe ﬁgure also shows the crossing of error contours at\\n-10\\n0\\n10\\n-300\\n-200\\n-100\\n0\\nq\\nerrors at bgrcm = 01234-order phi across k_a + k_b = 49\\nfive orders varphi estimation of moments, k=500 reference, 5-15 rounds feedback\\nFIG. 13:\\nGraph of the residual error measure ∂ˆΦ(ka,kb)/∂τ\\nacross the contour ka + kb = 49, with the series (94) trun-\\ncated at ﬁve successive orders of approximation ϕ(αmax) for\\nαmax = 0, . . . , 4 (color sequence bgrcm). Under perfect can-\\ncellation, the residual error at each order αmax would scale\\nas q2(αmax+1). This scaling is very closely approximated for\\nαmax = 1, and degrades due to imperfect control of asymp-\\ntotic expansions at higher orders, though the approximate\\nbehavior is attained.\\nCrossing of the error curves suggests\\na ﬁnite radius of convergence in q2 of the series (94), for\\n|q| /κ ∼0.38. The asymptotic expansion remains this good\\nor better for all larger κ.\\n|q| /κ ∼0.38, suggesting a ﬁnite radius of convergence\\nthat does not cover the entire (ka, kb) lattice.\\n20\\n40\\n60\\n80\\n100\\n20\\n40\\n60\\n80\\n100\\nk_a\\nk_b\\nerrors^(1/10) from a varphi^4th order approximation\\nFIG. 14:\\nLinear colormap of the error function ∂ˆΦ(ka,kb)/∂τ\\nfor αmax = 5, evaluated as the left-hand side of Eq. (95) which\\nwould equal zero for a stationary distribution. The deviation\\nfrom zero is raised to the 0.1 power, to produce a linear cross-\\nsection if the true residuals scale as q10.\\nFig. 14 is a contour plot of the errors ∂ˆΦ(ka,kb)/∂τ for\\n32\\nαmax = 5. The anti-diagonal cross section corresponds\\nto the outermost curve from Fig. 13, raised to the 0.1\\npower appropriate if the total error scales as q10. The\\nregion ka + kb ≲20 shows the divergence of the asymp-\\ntotic expansions already noted in Fig. 12. In the region\\nwhere the asymptotic expansions converge, the errors are\\nroughly constant along contours of ﬁxed |q| /κ for large\\nk.\\n[1] Lovász, László. Random walks on graphs: a survey. Com-\\nbinatorics, Paul Erdös is 80, 2:1–46, 1993.\\n[2] Bin Luo, Richard C. Wilson, and Edwin R. Hancock.\\nSpectral embedding of graphs.\\nPattern recognition,\\n36:2213–2230, 2003.\\n[3] Ann B. Lee and Larry Wasserman. Spectral connectiv-\\nity analysis. J. Am. Stat. Assoc., 105:1241–1255, 2008.\\narXiv:0811.0121v1.\\n[4] Wim Hordijk and Mike Steel.\\nDetecting autocat-\\nalytic, self-sustaining sets in chemical reaction systems.\\nJ. Theor. Biol., 227:451–461, 2004.\\n[5] Vincent Danos, Jérôme Feret, Walter Fontana, Russell\\nHarmer, and Jean Krivine. Rule-based modelling, sym-\\nmetries, reﬁnements. Formal methods in systems biology:\\nlecture notes in computer science, 5054:103–122, 2008.\\n[6] Russ Harmer, Vincent Danos, Jérôme Feret, Jean Kriv-\\nine, and Walter Fontana. Intrinsic information carriers\\nin combinatorial dynamical systems. Chaos, 20:037108,\\n2010.\\n[7] Jakob L. Andersen, Christoph Flamm, Daniel Merkle,\\nand Peter F. Stadler.\\nInferring chemical reaction\\npatterns using rule composition in graph grammars.\\nJ. Sys. Chem., 4:4:1–14, 2013.\\n[8] Jakob L. Andersen, Christoph Flamm, Daniel Merkle,\\nand Peter F. Stadler.\\nGeneric strategies for chemical\\nspace exploration.\\nInt. J. Comput. Biol. Drug Des.,\\n7:225–258, 2014.\\n[9] Martin\\nFeinberg.\\nLectures\\non\\nchemical\\nreaction\\nnetworks.\\nlecture\\nnotes,\\n1979.\\nhttps://crnt.osu.edu/LecturesOnReactionNetworks.\\n[10] Jeremy Gunawardena. Chemical reaction network the-\\nory for in-silico biologists.\\nlecture notes, June 2003.\\nvcp.med.harvard.edu/papers/crnt.pdf.\\n[11] John\\nC.\\nBaez\\nand\\nJacob\\nD.\\nBiamonte.\\nQuan-\\ntum\\ntechniques\\nfor\\nstochastic\\nmechanics.\\n2017.\\nmath.ucr.edu/home/baez/stoch_stable.pdf.\\n[12] Claude Berge. Graphs and Hypergraphs. North-Holland,\\nAmsterdam, rev. ed. edition, 1973.\\n[13] Jakob L. Andersen, Christoph Flamm, Daniel Merkle,\\nand Peter F. Stadler. Maximizing output and recogniz-\\ning autocatalysis in chemical reaction networks is NP-\\ncomplete. J. Sys. Chem., 3:1, 2012.\\n[14] David E. Metzler.\\nBiochemistry: The Chemical Reac-\\ntions of Living Cells. Academic Press, New York, second\\nedition, 2003.\\n[15] Bernhard O. Palsson.\\nSystems Biology.\\nCambridge\\nU. Press, Cambridge, MA, 2006.\\n[16] Hal L. Smith and Horst R. Thieme. Dynamical systems\\nand population persistance. Amer. Math. Soc., 2011.\\n[17] Linda J. S. Allen. An introduction to stochastic processes\\nwith applications to biology. Pearson, New Jersey, 2003.\\n[18] Martin Feinberg.\\nChemical reaction network struc-\\nture and the stability of complex isothermal reactors\\n– I. The deﬁciency zero and deﬁciency one theorems.\\nChem. Enc. Sci., 42:2229–2268, 1987.\\n[19] G. Craciun and M. Feinberg. Multiple equilibria in com-\\nplex chemical reaction networks.\\nSiam J. Appl. Math,\\n65:1526–1546, 2005.\\n[20] G. Craciun, Y. Tang, and M. Feinberg. Understanding\\nbistability in complex enzyme-driven reaction networks.\\nProc. Nat. Acad. Sci. USA, 103:8697–8702, 2006.\\n[21] Haixia Ji. Uniqueness of equilibria for complex chemical\\nreaction networks.\\nPhD thesis, Ohio State University,\\n2011.\\n[22] Badal Joshi and Anne Shiu.\\nA survey of methods for\\ndeciding whether a reaction network is multistationary.\\nMath. Model. Nat. Phenom., 10:47–67, 2015.\\n[23] Martin R. Evans and Tom Hanney. Non-equilibrium sta-\\ntistical mechanics of the zero-range process and related\\nmodels. J. Phys. A: Math. Gen., 38:R195–R239, 2005.\\n[24] David F. Anderson, George Craciun, and Thomas G.\\nKurtz.\\nProduct-form stationary distributions for de-\\nciency zero chemical reaction networks. Bull. Math. Bio.,\\n72:1947–1970, 2010.\\n[25] F. P. Kelly. Reversibility and stochastic networks. Wiley,\\nChichester, 1979.\\n[26] F. Horn and R. Jackson. General mass action kinetics.\\nArch. Rat. Mech. Anal, 47:81–116, 1972.\\n[27] Udo Seifert.\\nStochastic thermodynamics, ﬂuctuation\\ntheorems, and molecular machines.\\nRep. Prog. Phys.,\\n75:126001, 2012. arXiv:1205.4176v1.\\n[28] M. Doi. Second quantization representation for classical\\nmany-particle system. J. Phys. A, 9:1465–1478, 1976.\\n[29] M. Doi. Stochastic theory of diﬀusion-controlled reaction.\\nJ. Phys. A, 9:1479–, 1976.\\n[30] L. Peliti. Path-integral approach to birth-death processes\\non a lattice. J. Physique, 46:1469, 1985.\\n[31] L. Peliti. Renormalization of ﬂuctuation eﬀects in a+a →\\na reaction. J. Phys. A, 19:L365, 1986.\\n[32] Eric Smith and Supriya Krishnamurthy. Doi-peliti gen-\\nerating functionals, time-reversal, and dualities between\\ndynamics and inference for stochastic processes. page in\\npreparation, 2017.\\n[33] Supriya\\nKrishnamurthy\\nand\\nEric\\nSmith.\\nSolving\\nmoment hierarchies for chemical reaction networks.\\nPhys. Rev. Lett., page submitted, 2017.\\n[34] N. G. van Kampen. Stochastic Processes in Physics and\\nChemistry. Elsevier, Amsterdam, third edition, 2007.\\n[35] Eric Smith, Supriya Krishnamurthy, Walter Fontana,\\nand David C. Krakauer. Non-equilibrium phase transi-\\ntions in biomolecular signal transduction. Phys. Rev. E,\\n84:051917, 2011. PMID: 22181454.\\n[36] Eric Smith. Large-deviation principles, stochastic eﬀec-\\ntive actions, path entropies, and the structure and mean-\\ning of thermodynamic descriptions.\\nRep. Prog. Phys.,\\n74:046601, 2011. http://arxiv.org/submit/199903.\\n[37] Eric Smith and Supriya Krishnamurthy. Symmetry and\\nCollective Fluctuations in Evolutionary Games.\\nIOP\\nPress, Bristol, 2015.\\n[38] Daniel C. Mattis and M. Lawrence Glasser.\\nThe uses\\n33\\nof quantum ﬁeld theory in diﬀusion-limited reactions.\\nRev. Mod. Phys, 70:979–1001, 1998.\\n[39] J.\\nCardy.\\nField\\ntheory\\nand\\nnon-equilibrium\\nstatistical\\nmechanics.\\n1999.\\nhttp://www-\\nthphys.physics.ox.ac.uk/users/JohnCardy/home.html.\\n[40] David F. Anderson, Gheorghe Craciun, Manoj Gopalkr-\\nishnan, and Carsten Wiuf. Lyapunov functions, station-\\nary distributions, and non-equilibrium potential for reac-\\ntion networks. Bull. Math. Biol., 77:1744–1767, 2015.\\n[41] J. L. Cardy. Electron localisation in disordered systems\\nand classical solutions in ginzburg-landau ﬁeld theory.\\nJ. Phys. C, 11:L321 – L328, 1987.\\n[42] Sidney Coleman. Aspects of Symmetry. Cambridge, New\\nYork, 1985.\\n[43] Eric Smith and Harold J. Morowitz. The origin and na-\\nture of life on Earth: the emergence of the fourth geo-\\nsphere. Cambridge U. Press, London, 2016.\\n[44] Lennart Sjögren.\\nLecture Notes Stochastic Processes,\\nCh. 8: The Kramers problem and ﬁrst passage times.\\nhttp://physics.gu.se/∼frtbm/joomla/media/mydocs/LennartSjogren\\n2016. http://physics.gu.se/∼frtbm/joomla/index.php?option=com_c\\n[45] D Schnoerr, G. Sanguinetti, and R. Grima. Comparison\\nof diﬀerent moment-closure approximations for stochas-\\ntic chemical kinetics. J. Chem. Phys., 143:185101, 2015.\\n')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.\\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (2014–2019) ===\\n\\nSince its inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\\nIn 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundatio'), Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n=== Generative AI porn \"communities\" ===\\nPopular sites like civitai.com allow users to create, upload, and download fine-tuned versions of open source models of SDXL, Flux, and that are specifically designed for generating various pornographic scenes or effects.   \\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nSeveral US states are taking actions against using deepfake apps and sharing them on the internet. In 2024, San Fran')]\n"
     ]
    }
   ],
   "source": [
    "## Wikipedia\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs=WikipediaLoader(query=\"Generative AI\",load_max_docs=2).load()\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435a589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a9045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
